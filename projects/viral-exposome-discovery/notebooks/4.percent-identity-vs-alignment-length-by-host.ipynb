{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7a584b-0b3b-4f7b-b405-cc56addaeb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if super busted, shut down kernel, remove ~/.julia, start a REPL and reinstall IJulia, then restart here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349822c5-f469-4202-ada3-fcc561dd774f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if isfile(\"Project.toml\")\n",
    "    println(\"removing Project.toml...\")\n",
    "    rm(\"Project.toml\")\n",
    "end\n",
    "if isfile(\"Manifest.toml\")\n",
    "    println(\"removing Manifest.toml...\")\n",
    "    rm(\"Manifest.toml\")\n",
    "end\n",
    "ENV[\"LD_LIBRARY_PATH\"] = \"\"\n",
    "\n",
    "import Pkg\n",
    "Pkg.activate(\".\")\n",
    "# Pkg.update()\n",
    "# delete baseline environment???\n",
    "# Pkg.add(url=\"https://github.com/cjprybol/Mycelia.git\", rev=\"master\")\n",
    "# import Mycelia\n",
    "\n",
    "pkgs = [\n",
    "\"DataFrames\",\n",
    "\"uCSV\",\n",
    "\"ProgressMeter\"\n",
    "# \"StatsPlots\"\n",
    "]\n",
    "Pkg.add(pkgs)\n",
    "for pkg in pkgs\n",
    "    eval(Meta.parse(\"import $pkg\"))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909f13b4-7e9d-429b-8b83-33c99af653c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = joinpath(dirname(pwd()), \"data\")\n",
    "results_directory = joinpath(data_dir, \"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf0fdc-def3-44bf-b0ba-58ef182cdeb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# blastdbs_dir = \"$(homedir())/workspace/blastdbs\"\n",
    "# taxdump_dir = mkpath(joinpath(blastdbs_dir, \"taxdump\"))\n",
    "# taxdump_tar = joinpath(taxdump_dir, \"taxdump.tar.gz\")\n",
    "# if !isfile(taxdump_tar)\n",
    "#     run(`wget --quiet https://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz --directory-prefix=$(taxdump_dir)`)\n",
    "# end\n",
    "# if isempty(filter(x -> occursin(r\"\\.dmp$\", x), readdir(taxdump_dir)))\n",
    "#     run(`tar -xvzf $(taxdump_tar) --directory $(taxdump_dir)`)\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edc577c-07d3-41be-9cde-412d885f7dcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SRR_paths = filter(x -> !occursin(\".ipynb_checkpoints\", x), readdir(joinpath(data_dir, \"SRA\"), join=true))\n",
    "SRR_paths = filter(x -> isfile(joinpath(x, \"megahit\", \"final.contigs.fastg.gfa.fna\")), SRR_paths)\n",
    "\n",
    "# get all taxonids at or below virus\n",
    "# mamba create -n taxonkit -c bioconda taxonkit\n",
    "# wget -c ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz \n",
    "# tar -zxvf taxdump.tar.gz\n",
    "# mkdir -p $HOME/.taxonkit\n",
    "# cp names.dmp nodes.dmp delnodes.dmp merged.dmp $HOME/.taxonkit\n",
    "# --data-dir\n",
    "viral_tax_ids = Set(parse.(Int, filter(!isempty, readlines(`conda run -n taxonkit taxonkit list --ids 10239 --indent \"\"`))))\n",
    "# n_methods = 8\n",
    "data_dir = joinpath(dirname(pwd()), \"data\")\n",
    "RESULTS_DIR = mkpath(joinpath(data_dir, \"results\"))\n",
    "\n",
    "# SRR_paths = filter(x -> !occursin(\".ipynb_checkpoints\", x), readdir(joinpath(data_dir, \"SRA\"), join=true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5f1d4b-97ff-47b1-a8a5-23b3a9a24bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICTV_viral_metadata = \n",
    "# NCBI_viral_metadata = \n",
    "# VirusHostDB_viral_metadata = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11040716-d37d-4164-8eb5-d10bad3b13e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_viral_hits_table_file = joinpath(RESULTS_DIR, \"joint-viral-blast-hits.tsv\")\n",
    "# joint_viral_hits_table = DataFrames.DataFrame()\n",
    "# ProgressMeter.@showprogress for SRR_path in SRR_paths\n",
    "#     coverage_and_classification_reports = filter(x -> occursin(\"coverage-and-classification.tsv\", x), readdir(joinpath(SRR_path, \"blastn\")))\n",
    "#     @assert length(coverage_and_classification_reports) == 1\n",
    "#     coverage_and_classification_report = first(coverage_and_classification_reports)\n",
    "#     coverage_and_classification_report_path = joinpath(joinpath(SRR_path, \"blastn\", coverage_and_classification_report))\n",
    "#     coverage_and_classification_table = DataFrames.DataFrame(uCSV.read(coverage_and_classification_report_path, delim='\\t', header=1, types=String, allowmissing=true, encodings=Dict(\"\" => missing)))\n",
    "#     coverage_and_classification_table[!, \"sample_id\"] .= basename(SRR_path)\n",
    "#     coverage_and_classification_table[!, \"subject tax id\"] = parse.(Int, coverage_and_classification_table[!, \"subject tax id\"])\n",
    "#     coverage_and_classification_table = coverage_and_classification_table[map(x -> x in viral_tax_ids, coverage_and_classification_table[!, \"subject tax id\"]), :]\n",
    "#     append!(joint_viral_hits_table, coverage_and_classification_table, promote=true, cols=:union)\n",
    "# end\n",
    "# uCSV.write(joint_viral_hits_table_file, ifelse.(ismissing.(joint_viral_hits_table), \"\", joint_viral_hits_table), delim='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aef5fd1-05b5-4e2d-86da-19184e9e7464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a634472-8978-4dd0-8131-e0c0c5a7e5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_viral_hits_table_with_metadata_file = joinpath(RESULTS_DIR, \"joint-viral-blast-hits.with-host-metadata.tsv\")\n",
    "\n",
    "# metadata_dir = joinpath(dirname(pwd()), \"metadata\")\n",
    "# ncbi_virus_metadata_file = joinpath(metadata_dir, \"NCBI-virus-refseq.transformed.tsv\")\n",
    "# ncbi_virus_metadata = DataFrames.DataFrame(uCSV.read(ncbi_virus_metadata_file, header=1, delim='\\t'))\n",
    "# joint_viral_hits_table_with_metadata = DataFrames.innerjoin(\n",
    "#     joint_viral_hits_table,\n",
    "#     unique(ncbi_virus_metadata[!, [\"taxid\", \"host_is_vertebrate\", \"host_is_mammal\", \"host_is_primate\", \"host_is_human\"]]),\n",
    "#     on=\"subject tax id\" => \"taxid\"\n",
    "# )[!, DataFrames.Not(\"subject tax ids\")]\n",
    "# uCSV.write(joint_viral_hits_table_with_metadata_file, joint_viral_hits_table_with_metadata, delim='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9d3e60-f3c4-4015-8ed9-73f578749296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# readdir(metadata_dir)\n",
    "# \"VMR_MSL38_v1 - VMR MSL38 v1.transformed.tsv\"\n",
    "# \"NCBI-virus-refseq.transformed.tsv\"\n",
    "# \"virushostdb.transformed.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a187f9c-1a53-47d6-bde5-5fabedc60427",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# contig_info_table\n",
    "# blast_hits_top_hits_table\n",
    "# joint_lca_table\n",
    "# genomad_results\n",
    "# virsorter_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74adc05-d745-439a-8fc8-ea057dd5fff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = StatsPlots.scatter(\n",
    "    viral_blast_hits_joint_table[!, \"alignment length\"],\n",
    "    viral_blast_hits_joint_table[!, \"% identity\"],\n",
    "    title = \"aligment quality for conensus viral contigs\",\n",
    "    xlabel = \"alignment length\",\n",
    "    ylabel = \"% identity\",\n",
    "    legend=false,\n",
    "    size=(600, 400)\n",
    ")\n",
    "\n",
    "StatsPlots.savefig(p, joinpath(results_directory, \"figure-1.$(now_string).png\"))\n",
    "StatsPlots.savefig(p, joinpath(results_directory, \"figure-1.$(now_string).pdf\"))\n",
    "# StatsPlots.savefig(p, joinpath(RESULTS_DIR, \"figure-1.$(now_string).jpeg\"))\n",
    "# display(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec724484-40a0-4437-8f1b-2e8b247bbe23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "viral_blast_hits_joint_table_file = last(filter(x -> occursin(\"figure-1.viral_blast_hits_joint_table\", x), filter(x -> occursin(\".tsv\", x), readdir(results_directory, join=true))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f7c061-fa1c-4b7f-862b-a83823c33d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "viral_hits_joint_table = DataFrames.DataFrame(uCSV.read(viral_blast_hits_joint_table_file, delim='\\t', header=true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eef192-98c1-4fd4-97a5-fa4051abcd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top hits by length to annotate what we're seeing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4b6033-0c1f-45bd-bcc0-6ec77feb3e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess total # of viral families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab99b099-6ca8-4dc5-804a-1b89afb87f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess total # of viral genera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27848996-43f3-49c4-b435-a4eb2d23b9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the above by family and genera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c486a5-b2c3-479e-b08c-5eb3ed0babb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# StatsBase.describe(viral_blast_hits_joint_table[!, \"alignment length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b2d7d4-8637-483c-8cce-e9e49633a7ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# viral_blast_hits_joint_table[viral_blast_hits_joint_table[!, \"alignment length\"] .>= 100, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99a7f02-635f-41d2-be5a-4e0b0c9c7f10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# taxdump_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de009c09-c482-4366-9963-1bf802a76f89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "taxid = 9606"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ac23cb-65cd-4150-9791-d65718f02ee8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "taxid, ranks, taxids, split(strip(read(pipeline(`echo $taxid`, `conda run --no-capture-output -n taxonkit taxonkit --data-dir $(taxdump_dir) lineage --show-lineage-ranks --show-lineage-taxids --show-rank`), String)), '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4803a7-620b-4925-b549-54d968b0c135",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda run --no-capture-output -n taxonkit taxonkit --data-dir $HOME/workspace/blastdbs/taxdump list --indent \"\" --ids 1 > taxids.txt\n",
    "\n",
    "\n",
    "conda run --no-capture-output -n taxonkit taxonkit --data-dir $HOME/workspace/blastdbs/taxdump lineage --show-rank taxids.txt > taxon-lineages.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7df9e7-f4ff-49cb-836b-fc6c5e380066",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function taxid_to_ranked_lineage(taxid)\n",
    "\n",
    "    taxid_lineage_reformatted = join(split(taxid_lineage, ';'), '\\n')\n",
    "    d = Dict()\n",
    "    for line in readlines(pipeline(`echo $(taxid_lineage_reformatted)`, `conda run --no-capture-output -n taxonkit taxonkit --data-dir $(taxdump_dir) name2taxid --show-rank`))\n",
    "        rank_name, rank_taxid, rank = string.(split(line, '\\t'))\n",
    "        d[rank] = Dict(\"name\" => rank_name, \"taxid\" => parse(Int, rank_taxid))\n",
    "    end\n",
    "    return d\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8d67f5-50ed-4dc6-9eef-90e04b0173b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9221a61-aea1-49d0-b228-56e41bb3436d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # conda create -n taxonkit -c bioconda taxonkit\n",
    "# taxid = 353768\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935351e5-e9f4-4099-a70b-19baa79637bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "taxon_lineages = Dict()\n",
    "ProgressMeter.@showprogress for taxid in unique(viral_blast_hits_joint_table[!, \"subject tax id\"])\n",
    "    taxon_lineages[taxid] = taxid_to_ranked_lineage(taxid)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5546bcc-7917-4b40-8e15-8d25ec59ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taxonkit list --ids 1 --indent \"\" | taxonkit lineage --show-lineage-ranks --show-lineage-taxids > t\n",
    "# taxonkit list --ids 1 --indent \"\" \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "echo 9606 | conda run --no-capture-output -n taxonkit taxonkit --data-dir $HOME/workspace/blastdbs/taxdump lineage --show-lineage-ranks --show-lineage-taxids --show-rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1fb259-be63-4822-a07d-762a920cbac8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "  -R, --show-lineage-ranks    appending ranks of all levels\n",
    "  -t, --show-lineage-taxids   appending lineage consisting of taxids\n",
    "  -n, --show-name             appending scientific name\n",
    "  -r, --show-rank             appending rank of taxids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef313740-89bc-4789-9ab7-9652914e2bfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# contig_info_table[!, \"viral_classification_count\"] = map(contig -> get(contig_support_counts, contig, 0), contig_info_table[!, \"Contig\"])\n",
    "# contig_info_table[!, \"viral_classification_percent\"] = round.((contig_info_table[!, \"viral_classification_count\"] ./ n_methods) .* 100, digits=1)\n",
    "# end\n",
    "# sample_summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ce2b12-102a-40c7-931d-cac3e666daa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SRR_paths\n",
    "# SRR_path = rand(SRR_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aebdec-a439-460c-8228-69e2dfed95f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IMGVR_sam = joinpath(SRR_path, \"megahit\", \"final.contigs.fastg.gfa.viral.fna.IMGVR_all_nucleotides-high_confidence.fna.gz.sam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e85679-4efb-4a12-8d60-2fd923503a48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# uCSV.read(IMGVR_sam, delim='\\t', typedetectrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d54aa2-e91f-4ba7-8bd6-da27493da712",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reader = open(XAM.SAM.Reader, IMGVR_sam)\n",
    "# # Iterate over BAM records.\n",
    "# for record in reader\n",
    "#     # `record` is a BAM.Record object.\n",
    "#     if XAM.SAM.ismapped(record)\n",
    "#         # Print the mapped position.\n",
    "#         println(XAM.SAM.refname(record), ':', XAM.SAM.position(record))\n",
    "#     end\n",
    "# end\n",
    "\n",
    "# # Close the BAM file.\n",
    "# close(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6655bbe-5035-4621-89ba-4335dba973fa",
   "metadata": {},
   "source": [
    "Based on these results, we will go for a 3x confirmation via targetted blast where the sequence must have a high confidence match for a viral hit using:\n",
    "- ref_viruses_rep_genomes_blastn\n",
    "- ref_viruses_rep_genomes_dcmegablast\n",
    "- nt_viral_validation_megablast\n",
    "\n",
    "Full nt database attempts to validate viral contigs came back primarily with bacterial artificial chromosomes associated with human cell lines that did not seem like valuable hits to us. We also didn't expect to find *novel* viruses by megablasting against the ref_viruses_rep_genomes, given that it is a limited, representative set of all known viruses and the official description of the algorithm is \"Traditional megablast used to find very similar (e.g., intraspecies or closely related species) sequences\"\n",
    "https://www.ncbi.nlm.nih.gov/books/NBK569839/\n",
    "\n",
    "Because of this, we felt that getting a multiple redudant hits using the most flexibile algorithm (blastn) against the highest quality blast db (ref_viruses_rep_genomes), a less flexible, but still cross-species algorithm (dc-megablast: Discontiguous megablast used to find more distant (e.g., interspecies) sequences), and finally a 3rd validation hit against a potentially lower quality due to less manual curation viral database (nt_viral) using the strictest alogrithm, megablast.\n",
    "\n",
    "Because of the low concordance of Kraken classifications to blast classifications, we consider the calls with much less weight than the classified contigs, but include them because the data was generated and it may prove informative to others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6686068b-a98d-4a1e-9719-c1891057396b",
   "metadata": {},
   "source": [
    "As an update to the above, we found that we were unable to perform the `nt_viral_validation_megablast` in a reasonable amount of time, so we will be dropping that requirement.\n",
    "\n",
    "Instead, we will perform a final re-assembly and then perform the final validation blast and protein-level classification on that final assembly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3da95f-d7b7-494f-8022-960a9727b241",
   "metadata": {},
   "source": [
    "The following was a manual, indepth analysis of the first sample that we used to inform our validation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2e4b7f-56b3-4ea3-9f93-a2541f067dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract contigs that came back as viral in the targetted screen\n",
    "# # blast back against NCBI nt and confirm they are still viral\n",
    "# SRR_path = first(SRR_paths)\n",
    "# SRR = basename(SRR_path)\n",
    "\n",
    "# # ref_viruses_ref_genomes_blast_report = joinpath(SRR_path, \"blastn\", \"final.contigs.fastg.gfa.fna.blastn.ref_viruses_rep_genomes.blastn.txt\")\n",
    "# # ref_viruses_ref_genomes_blast_results = Mycelia.parse_blast_report(ref_viruses_ref_genomes_blast_report)\n",
    "# # possible_viral_contigs = Set(unique(ref_viruses_ref_genomes_blast_results[!, \"query id\"]))\n",
    "# # assembled_fasta = joinpath(SRR_path, \"megahit\", \"final.contigs.fastg.gfa.fna\")\n",
    "# # viral_fasta = replace(assembled_fasta, \".fna\" => \".potential_viral_contigs.fna\")\n",
    "# # potential_viral_records = filter(x -> FASTX.identifier(x) in possible_viral_contigs, collect(Mycelia.open_fastx(assembled_fasta)))\n",
    "# # open(viral_fasta, \"w\") do io\n",
    "# #     fastx_io = FASTX.FASTA.Writer(io)\n",
    "# #     for record in potential_viral_records\n",
    "# #         write(fastx_io, record)\n",
    "# #     end\n",
    "# #     close(fastx_io)\n",
    "# # end\n",
    "\n",
    "# # potential_viral_records\n",
    "\n",
    "\n",
    "# taxon_id_to_kingdom_map = Dict{Int, String}()\n",
    "# kingdom_to_taxon_id_map = Dict(\n",
    "#     \"Viruses\" => 10239,\n",
    "#     \"Archaea\" => 2157,\n",
    "#     \"Bacteria\" => 2,\n",
    "#     \"Eukaryota\" => 2759,\n",
    "#     \"Other\" => 28384,\n",
    "#     \"Unclassified\" => 12908\n",
    "# )\n",
    "# ProgressMeter.@showprogress for (kingdom, taxon_id) in kingdom_to_taxon_id_map\n",
    "#     for child_taxon_id in parse.(Int, filter(!isempty, readlines(`taxonkit list --data-dir $(taxdump_dir) --ids $(taxon_id) --indent=\"\"`)))\n",
    "#         taxon_id_to_kingdom_map[child_taxon_id] = kingdom\n",
    "#     end\n",
    "# end\n",
    "\n",
    "# ref_viruses_rep_genomes_blastn_contigs = unique(ref_viruses_rep_genomes_blastn_results[!, \"query id\"])\n",
    "\n",
    "# ref_viruses_rep_genomes_dcmegablast_contigs = unique(ref_viruses_rep_genomes_dcmegablast_results[!, \"query id\"])\n",
    "\n",
    "# # # 8 non-overlapping hits!\n",
    "# # union(ref_viruses_rep_genomes_dcmegablast_contigs, ref_viruses_rep_genomes_blastn_contigs)\n",
    "# # # these all have pretty low e-values, I'm not going to worry about them\n",
    "# # dcmegablast_only = setdiff(ref_viruses_rep_genomes_dcmegablast_contigs, ref_viruses_rep_genomes_blastn_contigs)\n",
    "# # ref_viruses_rep_genomes_dcmegablast_results[map(x -> x in dcmegablast_only, ref_viruses_rep_genomes_dcmegablast_results[!, \"query id\"]), :]\n",
    "\n",
    "# ref_viruses_rep_genomes_megablast_contigs = unique(ref_viruses_rep_genomes_megablast_results[!, \"query id\"])\n",
    "# # nt_viral_validation_megablast_results\n",
    "# # nt_validation_megablast_results\n",
    "# # all megablast hits are subset of blastn hits\n",
    "# # union(ref_viruses_rep_genomes_megablast_contigs, ref_viruses_rep_genomes_blastn_contigs)\n",
    "\n",
    "# nt_viral_validation_megablast_contigs = unique(nt_viral_validation_megablast_results[!, \"query id\"])\n",
    "# # nt_validation_megablast_results\n",
    "\n",
    "# # union(nt_viral_validation_megablast_contigs, ref_viruses_rep_genomes_blastn_contigs)\n",
    "# # map(x -> x in viral_taxon_ids, nt_viral_validation_megablast_results[!, \"subject tax id\"])\n",
    "\n",
    "# taxon_id_to_kingdom_map\n",
    "\n",
    "# # only 12 contigs are still considered viral contigs after mapping to nt complete, but the hits aren't very convicing (bacterial artificial chromosomes?)\n",
    "# nt_validation_megablast_contigs = unique(nt_validation_megablast_results[map(x -> get(taxon_id_to_kingdom_map, x, \"\") == \"Viruses\", nt_validation_megablast_results[!, \"subject tax id\"]), \"query id\"])\n",
    "\n",
    "# full_contig_set = union(nt_validation_megablast_contigs, nt_viral_validation_megablast_contigs, ref_viruses_rep_genomes_blastn_contigs, ref_viruses_rep_genomes_dcmegablast_contigs, ref_viruses_rep_genomes_megablast_contigs)\n",
    "\n",
    "# # get all of the reads mapping to each\n",
    "\n",
    "# bam_file = joinpath(SRR_path, \"megahit\", \"final.contigs.fastg.gfa.fna.bwa.bam\")\n",
    "# # bamfile = first(filter(x -> occursin(r\"\\.bam$\", x), readdir(joinpath(SRR_dir, \"megahit\"), join=true)))\n",
    "\n",
    "# # implement as the following nested dictionary\n",
    "# # contigs => reads => taxon_id\n",
    "\n",
    "# function generate_contig_to_reads_map(bamfile, contigs_of_interest)\n",
    "#     contigs_to_reads_map = Dict(contig => Set{String}() for contig in contigs_of_interest)\n",
    "#     # reads_of_interest = Set{String}()\n",
    "#     reader = open(XAM.BAM.Reader, bamfile)\n",
    "#     for record in reader\n",
    "#         if XAM.BAM.ismapped(record) && (XAM.BAM.refname(record) in contigs_of_interest)\n",
    "#             push!(contigs_to_reads_map[XAM.BAM.refname(record)], XAM.BAM.tempname(record))\n",
    "#         end\n",
    "#     end\n",
    "#     close(reader)\n",
    "#     return contigs_to_reads_map\n",
    "# end\n",
    "\n",
    "# # 1200 seconds\n",
    "# @time contigs_to_reads_map = generate_contig_to_reads_map(bam_file, full_contig_set)\n",
    "\n",
    "# function read_kraken_output(kraken_output)\n",
    "#     # read_kraken_report\n",
    "#     header = [\n",
    "#         \"classification status\",\n",
    "#         \"sequence ID\",\n",
    "#         \"taxon ID\",\n",
    "#         \"sequence length\",\n",
    "#         \"LCA mappings\"\n",
    "#     ]\n",
    "#     data, _ = uCSV.read(IOBuffer(join(filtered_lines, '\\n')), delim='\\t')\n",
    "#     return DataFrames.DataFrame(data, header)\n",
    "# end\n",
    "\n",
    "# # readdir(joinpath(SRR_path, \"kraken\"))\n",
    "\n",
    "# reads_of_interest = reduce(union, values(contigs_to_reads_map))\n",
    "\n",
    "# kraken_output = last(filter(x -> occursin(r\"\\.kraken-output\\.tsv\", x), readdir(joinpath(SRR_path, \"kraken\"), join=true)))\n",
    "# if occursin(r\"\\.gz\", kraken_output)\n",
    "#     kraken_buffer = open(`gzip -dc $(kraken_output)`)\n",
    "# else\n",
    "#     kraken_buffer = open(kraken_output)\n",
    "# end\n",
    "\n",
    "# filtered_lines = String[]\n",
    "# for line in eachline(kraken_buffer)\n",
    "#     split_line = split(line, '\\t')\n",
    "#     if split_line[2] in reads_of_interest\n",
    "#         push!(filtered_lines, line)\n",
    "#     end\n",
    "# end\n",
    "# close(kraken_buffer)\n",
    "\n",
    "# read_classifications = read_kraken_output(IOBuffer(join(filtered_lines, '\\n')))\n",
    "# read_classifications[!, \"parsed taxon ID\"] = map(x -> match(r\"\\(taxid (\\d+)\\)\", x).captures[1], read_classifications[!, \"taxon ID\"])\n",
    "# read_classifications\n",
    "\n",
    "# read_classifications_map = Dict(row[\"sequence ID\"] => parse(Int, row[\"parsed taxon ID\"]) for row in DataFrames.eachrow(read_classifications))\n",
    "\n",
    "# contigs_to_taxon_counts_map = Dict()\n",
    "# for (contig, reads) in contigs_to_reads_map\n",
    "#     contigs_to_taxon_counts_map[contig] = StatsBase.countmap(get(taxon_id_to_kingdom_map, read_classifications_map[read], \"Unclassified\") for read in reads)\n",
    "# end\n",
    "# contigs_to_taxon_counts_map\n",
    "\n",
    "# contigs_to_taxon_proportions_map = Dict()\n",
    "# for (contig, taxon_counts) in contigs_to_taxon_counts_map\n",
    "#     total_count = sum(values(taxon_counts))\n",
    "#     contigs_to_taxon_proportions_map[contig] = Dict(kingdom => count / total_count for (kingdom, count) in taxon_counts)\n",
    "# end\n",
    "# contigs_to_taxon_proportions_map\n",
    "\n",
    "# # ref_viruses_rep_genomes_blastn_results\n",
    "# # ref_viruses_rep_genomes_dcmegablast_results\n",
    "# # ref_viruses_rep_genomes_megablast_results\n",
    "# # nt_viral_validation_megablast_results\n",
    "# # nt_validation_megablast_results\n",
    "\n",
    "# contig_classification_results = \n",
    "# DataFrames.DataFrame(\n",
    "#     union(\n",
    "#         DataStructures.OrderedDict(\"Contig\" => String[]),\n",
    "#         DataStructures.OrderedDict(k => Float64[] for k in keys(kingdom_to_taxon_id_map)),\n",
    "#         # these are ordered by most hits to fewest hits\n",
    "#         DataStructures.OrderedDict(db_algorithm => Bool[] for db_algorithm in [\"ref_viruses_rep_genomes_blastn\", \"ref_viruses_rep_genomes_dcmegablast\", \"nt_viral_validation_megablast\", \"ref_viruses_rep_genomes_megablast\", \"nt_validation_megablast\"])\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# for (contig, taxon_proportions) in contigs_to_taxon_proportions_map\n",
    "#     row = Dict{Any, Any}(\"Contig\" => contig)\n",
    "#     for k in keys(kingdom_to_taxon_id_map)\n",
    "#         row[k] = get(taxon_proportions, k, 0.0)\n",
    "#     end\n",
    "#     row[\"ref_viruses_rep_genomes_blastn\"] = contig in ref_viruses_rep_genomes_blastn_contigs\n",
    "#     row[\"nt_viral_validation_megablast\"] = contig in nt_viral_validation_megablast_contigs\n",
    "#     row[\"ref_viruses_rep_genomes_dcmegablast\"] = contig in ref_viruses_rep_genomes_dcmegablast_contigs\n",
    "#     row[\"ref_viruses_rep_genomes_megablast\"] = contig in ref_viruses_rep_genomes_megablast_contigs\n",
    "#     row[\"nt_validation_megablast\"] = contig in nt_validation_megablast_contigs\n",
    "#     push!(contig_classification_results, row)\n",
    "# end\n",
    "# contig_classification_results[!, \"top_kingdom\"] .= \"\"\n",
    "# for (i, row) in enumerate(DataFrames.eachrow(contig_classification_results))\n",
    "#     max_hit = \"\"\n",
    "#     max_value = 0.0\n",
    "#     for k in keys(kingdom_to_taxon_id_map)\n",
    "#         if row[k] > max_value\n",
    "#             max_value = row[k]\n",
    "#             max_hit = k\n",
    "#         end\n",
    "#     end\n",
    "#     contig_classification_results[i, \"top_kingdom\"] = max_hit\n",
    "# end\n",
    "\n",
    "# m = Int.(Matrix(\n",
    "#     contig_classification_results[!, [\n",
    "#         \"ref_viruses_rep_genomes_blastn\",\n",
    "#         \"ref_viruses_rep_genomes_dcmegablast\",\n",
    "#         \"nt_viral_validation_megablast\",\n",
    "#         \"ref_viruses_rep_genomes_megablast\",\n",
    "#         \"nt_validation_megablast\"\n",
    "#     ]\n",
    "# ]))\n",
    "\n",
    "# contig_classification_results[!, \"blast_hits\"] = map(r -> sum(r), eachrow(m))\n",
    "\n",
    "# contig_classification_results\n",
    "\n",
    "# # eukaryotic top hit - kraken is asserting these are human contamination\n",
    "# sum(contig_classification_results[!, \"Eukaryota\"])\n",
    "\n",
    "# # novel sequences are second hit, this is exciting!\n",
    "# sum(contig_classification_results[!, \"Unclassified\"])\n",
    "\n",
    "# sum(contig_classification_results[!, \"Bacteria\"])\n",
    "\n",
    "# sum(contig_classification_results[!, \"Other\"])\n",
    "\n",
    "# sum(contig_classification_results[!, \"Viruses\"])\n",
    "\n",
    "# sum(contig_classification_results[!, \"Archaea\"])\n",
    "\n",
    "# contig_classification_results_summary = contig_classification_results[!, [\"Contig\", \"top_kingdom\", \"blast_hits\"]]\n",
    "\n",
    "# contig_classification_results_summary\n",
    "\n",
    "# StatsBase.countmap(contig_classification_results_summary[!, \"blast_hits\"])\n",
    "\n",
    "# contig_classification_results_summary[contig_classification_results_summary[!, \"blast_hits\"] .== 5, :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
