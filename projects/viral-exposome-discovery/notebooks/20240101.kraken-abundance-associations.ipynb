{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f670684-0b08-4679-b2bd-f5851ec0d2d8",
   "metadata": {},
   "source": [
    "The objective of this notebook is to:\n",
    "- join all of the kraken results with the sample metadata\n",
    "- subset to samples with metadata\n",
    "- collapse P3+ into Other\n",
    "- do PCA, coloring by metadata\n",
    "- do GLM modelling to determine if any have significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28afd0da-a1e3-48ad-a12a-8bbb3a8d19c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# don't try and install plotting libraries without this\n",
    "# can set in ~/.local/share/jupyter/kernels/\n",
    "@assert ENV[\"LD_LIBRARY_PATH\"] == \"\"\n",
    "import Pkg\n",
    "pkgs = [\n",
    "    \"Revise\",\n",
    "    \"DataFrames\",\n",
    "    \"StatsBase\",\n",
    "    \"StatsPlots\",\n",
    "    \"uCSV\",\n",
    "    \"ProgressMeter\",\n",
    "    \"Distances\",\n",
    "    \"Clustering\",\n",
    "    \"Colors\",\n",
    "    \"MultivariateStats\",\n",
    "    \"Dates\",\n",
    "    \"CategoricalArrays\",\n",
    "    \"GLM\",\n",
    "    \"Statistics\"\n",
    "]\n",
    "# Pkg.add(pkgs)\n",
    "for pkg in pkgs\n",
    "    eval(Meta.parse(\"import $pkg\"))\n",
    "end\n",
    "import Mycelia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc54d53-21ec-4098-b1c9-8c8e67d4193a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = joinpath(dirname(pwd()), \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c29934-2fb9-44fb-968e-1cf5a8c907ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_dir = joinpath(data_dir, \"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d349c0-1a92-4233-8ef5-a27b6b6af185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in metadata\n",
    "metadata_dir = joinpath(dirname(pwd()), \"metadata\")\n",
    "\n",
    "exposome_environmental_data = DataFrames.DataFrame(uCSV.read(\n",
    "    joinpath(metadata_dir, \"metadata_exposome.rds.tsv\"),\n",
    "    delim='\\t',\n",
    "    header=1,\n",
    "    typedetectrows=300\n",
    "))\n",
    "\n",
    "joint_sample_metadata = DataFrames.DataFrame(uCSV.read(\n",
    "    joinpath(metadata_dir, \"exposome/joint_sample_metadata.tsv\"),\n",
    "    delim='\\t',\n",
    "    header=1,\n",
    "    typedetectrows=300\n",
    "))\n",
    "\n",
    "@assert joint_sample_metadata[!, \"Library Name\"] == joint_sample_metadata[!, \"LibraryName\"]\n",
    "joint_metadata = DataFrames.innerjoin(\n",
    "    joint_sample_metadata,\n",
    "    exposome_environmental_data,\n",
    "    on=\"Library Name\" => \"samplenames\");\n",
    "\n",
    "\n",
    "# # recode P3 and beyond to Other, since they don't have enough samples to do much analysis on\n",
    "# joint_metadata[!, \"aownership\"] = map(x -> x in Set([\"P1\", \"P2\"]) ? x : \"Others\", joint_metadata[!, \"aownership\"])\n",
    "\n",
    "joint_metadata[!, \"date.start\"] = Dates.Date.(joint_metadata[!, \"date.start\"], \"yyyy-mm-dd\")\n",
    "joint_metadata[!, \"date.end\"] = Dates.Date.(joint_metadata[!, \"date.end\"], \"yyyy-mm-dd\")\n",
    "# use this if we want to unified axis across all participants, which I don't think we do\n",
    "# joint_metadata[!, \"date.start_relative\"] = joint_metadata[!, \"date.start\"] .- first(joint_metadata[!, \"date.start\"])\n",
    "# joint_metadata[!, \"date.end_relative\"] = joint_metadata[!, \"date.end\"] .- first(joint_metadata[!, \"date.start\"])\n",
    "joint_metadata[!, \"duration\"] = joint_metadata[!, \"date.end\"] .- joint_metadata[!, \"date.start\"]\n",
    "\n",
    "joint_metadata[!, \"temperature\"] = something.(tryparse.(Float64, joint_metadata[!, \"temperature\"]), missing)\n",
    "joint_metadata[!, \"humid\"] = something.(tryparse.(Float64, joint_metadata[!, \"humid\"]), missing)\n",
    "joint_metadata[!, \"particle\"] = something.(tryparse.(Float64, joint_metadata[!, \"particle\"]), missing)\n",
    "joint_metadata[!, \"latitude\"] = something.(tryparse.(Float64, joint_metadata[!, \"latitude\"]), missing)\n",
    "joint_metadata[!, \"longitude\"] = something.(tryparse.(Float64, joint_metadata[!, \"longitude\"]), missing)\n",
    "joint_metadata[!, \"altitude\"] = something.(tryparse.(Float64, joint_metadata[!, \"altitude\"]), missing)\n",
    "DataFrames.rename!(joint_metadata, \"date.month\" => \"month\")\n",
    "\n",
    "joint_metadata = DataFrames.dropmissing(joint_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92cbe00-1abc-4289-a05a-3c7f3b62225e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_paths = sort(joinpath.(data_dir, \"SRA\", joint_metadata[!, \"Run\"]))\n",
    "kraken_db = \"k2_pluspfp\"\n",
    "kraken_db_regex = Regex(\"$(kraken_db)_\\\\d{8}\")\n",
    "kraken_reports = map(path ->\n",
    "    first(filter(x -> occursin(kraken_db_regex, x) && occursin(r\"kraken-report\\.tsv$\", x), readdir(joinpath(path, \"kraken\"), join=true))),\n",
    "    sample_paths)\n",
    "\n",
    "# create a full joint table so that we can subset dynamically down below without needing to re-read all of them over and over again\n",
    "joint_report_table = DataFrames.DataFrame()\n",
    "ProgressMeter.@showprogress for kraken_report in kraken_reports\n",
    "    report_table = Mycelia.read_kraken_report(kraken_report)\n",
    "    report_table[!, \"report\"] .= basename(kraken_report)\n",
    "    append!(joint_report_table, report_table)\n",
    "end\n",
    "joint_report_table[!, \"taxon\"] = map(row -> string(row[\"ncbi_taxonid\"]) * \"_\" * row[\"scientific_name\"], DataFrames.eachrow(joint_report_table))\n",
    "joint_report_table[!, \"sample_identifier\"] = string.(first.(split.(joint_report_table[!, \"report\"], '.')))\n",
    "joint_report_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b607ee4f-7cf7-499e-866b-6d771b9358ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StatsPlots.plotlyjs()\n",
    "#default\n",
    "# StatsPlots.gr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca351ce-4f13-4f12-b0a7-2d1946cd93c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model_results = DataFrames.DataFrame(\n",
    "    rank = String[],\n",
    "    taxon = String[],\n",
    "    feature = String[],\n",
    "    rawpvalue = Float64[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793486ed-2657-4eb5-be8f-640193c6ccdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "taxon_levels = Mycelia.list_ranks()\n",
    "viral_tax_ids = Mycelia.list_subtaxa(10239)\n",
    "\n",
    "# rank_level = 3\n",
    "# rank_level = 4\n",
    "# rank_level = 5\n",
    "# rank_level = 6\n",
    "# rank_level = 7\n",
    "# rank_level = 8\n",
    "\n",
    "for rank_level in 3:8\n",
    "\n",
    "    (taxon_index, taxon_level) = collect(enumerate(taxon_levels))[rank_level]\n",
    "    println(\"$(taxon_index) - $(taxon_level)\")\n",
    "    rank_table = Mycelia.list_rank(taxon_level)\n",
    "\n",
    "    # filter the kraken results to only those at this level\n",
    "    taxids_at_this_rank = Set(rank_table[!, \"taxid\"])\n",
    "    rank_report_table = joint_report_table[map(x -> x in taxids_at_this_rank, joint_report_table[!, \"ncbi_taxonid\"]), :]\n",
    "    rank_joint_table = DataFrames.innerjoin(joint_metadata, rank_report_table, on=\"Run\" => \"sample_identifier\")\n",
    "    # find all columns with invariant metadata, and drop them\n",
    "    rank_joint_table = rank_joint_table[!, [n for n in names(rank_joint_table) if length(unique(rank_joint_table[!, n])) > 1]]\n",
    "\n",
    "    # println(\"[\")\n",
    "    # for n in names(rank_joint_table)\n",
    "    #     println(\"\\t\\\"$(n)\\\",\")\n",
    "    # end\n",
    "    # println(\"]\")\n",
    "\n",
    "    columns_of_interest = [\n",
    "        \"Run\",\n",
    "        \"altitude\",\n",
    "        \"geo_loc_name\",\n",
    "        \"location\",\n",
    "        \"geo\",\n",
    "        \"geo2\",\n",
    "        \"duration\",\n",
    "        \"month\",\n",
    "        \"season\",\n",
    "        \"particle\",\n",
    "        \"temperature\",\n",
    "        \"humid\",\n",
    "        \"weekend\",\n",
    "        \"aownership\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"ncbi_taxonid\",\n",
    "        \"scientific_name\",\n",
    "        \"taxon\",\n",
    "        \"number_of_fragments_at_or_below_taxon\",\n",
    "    ]\n",
    "\n",
    "    rank_joint_table = rank_joint_table[!, columns_of_interest]\n",
    "\n",
    "    # viral only\n",
    "    rank_joint_viral_table = rank_joint_table[map(x -> x in viral_tax_ids, rank_joint_table[!, \"ncbi_taxonid\"]), :]\n",
    "    # require at least 3 reads of support\n",
    "    rank_joint_viral_table = rank_joint_viral_table[rank_joint_viral_table[!, \"number_of_fragments_at_or_below_taxon\"] .>= 3, :]\n",
    "\n",
    "    normalized_rank_joint_viral_table = DataFrames.DataFrame()\n",
    "    for gdf in DataFrames.groupby(rank_joint_viral_table, \"Run\")\n",
    "        gdf[!, \"proportion_of_fragments_at_or_below_taxon\"] = gdf[!, \"number_of_fragments_at_or_below_taxon\"] ./ sum(gdf[!, \"number_of_fragments_at_or_below_taxon\"])\n",
    "        append!(normalized_rank_joint_viral_table, gdf)\n",
    "    end\n",
    "    normalized_rank_joint_viral_table = normalized_rank_joint_viral_table[!, DataFrames.Not(\"number_of_fragments_at_or_below_taxon\")]\n",
    "    normalized_rank_joint_viral_table = normalized_rank_joint_viral_table[!, DataFrames.Not([\"ncbi_taxonid\", \"scientific_name\", \"Run\"])]\n",
    "\n",
    "    categorical_columns = [\n",
    "        \"geo_loc_name\",\n",
    "        \"location\",\n",
    "        \"geo2\",\n",
    "        \"month\",\n",
    "        \"season\",\n",
    "        \"weekend\",\n",
    "        \"aownership\"\n",
    "    ]\n",
    "    for categorical_column in categorical_columns\n",
    "        normalized_rank_joint_viral_table[!, categorical_column] = CategoricalArrays.categorical(normalized_rank_joint_viral_table[!, categorical_column])\n",
    "    end\n",
    "    normalized_rank_joint_viral_table\n",
    "\n",
    "    taxon_tables = DataFrames.groupby(normalized_rank_joint_viral_table, \"taxon\")\n",
    "\n",
    "    # # Define the model\n",
    "    # # geo_loc_name\n",
    "    # # weekend\n",
    "    # # aownership\n",
    "    for taxon_table in taxon_tables\n",
    "        try\n",
    "            # model = GLM.lm(GLM.@formula(proportion_of_fragments_at_or_below_taxon ~ altitude + geo_loc_name + location + geo2 + duration + month + season + particle + temperature + humid + weekend +  aownership + latitude + longitude ), taxon_table)\n",
    "            model = GLM.lm(GLM.@formula(proportion_of_fragments_at_or_below_taxon ~ geo2 + season + aownership), taxon_table)\n",
    "\n",
    "            coeftable = GLM.coeftable(model)\n",
    "\n",
    "            for (feature, pval) in zip(coeftable.rownms, coeftable.cols[coeftable.pvalcol])\n",
    "                row = (\n",
    "                    rank = \"$(taxon_index)-$(taxon_level)\",\n",
    "                    taxon = taxon_table[1, \"taxon\"],\n",
    "                    feature = feature,\n",
    "                    rawpvalue = pval\n",
    "                    )\n",
    "                push!(linear_model_results, row)\n",
    "            end\n",
    "        catch\n",
    "            display(DataFrames.nrow(taxon_table))\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58669c2-8a5e-42e9-ac90-d30f0aea014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model_results[!, \"adjusted_pvalue\"] .= linear_model_results[!, \"rawpvalue\"] .* DataFrames.nrow(linear_model_results)\n",
    "sort!(linear_model_results, \"adjusted_pvalue\")\n",
    "linear_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58460e18-6928-46f5-bf0a-a43eb4ab2144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uCSV.write(joinpath(results_dir, \"20240101.kraken-abundance-associations.linear-modelling-results.tsv\"), linear_model_results, delim='\\t')\n",
    "uCSV.write(joinpath(results_dir, \"20240101.kraken-abundance-associations.linear-modelling-results.1.tsv\"), linear_model_results, delim='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889f00a2-3cf9-411a-9132-d296025ed6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bonferroni correct by # of tests\n",
    "# sort by p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f459b6-0931-4b30-888a-f71c01ebdb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model\n",
    "# predictions = GLM.predict(model, taxon_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.7",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
