{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0523df2e-b25a-4efe-8869-87b3c41d123b",
   "metadata": {},
   "source": [
    "The objective of this notebook is to cross-validate the 2 different NCBI blast mappings meant to identify viral contigs with read-level classifications produced by Kraken\n",
    "\n",
    "Contigs will be filtered to only those that are classified as viral by NCBI blast against the \"ref_viruses_rep_genomes\" blast database using 2 different algorithms\n",
    "- blastn\n",
    "    - this is expected to be overly generous\n",
    "    - any contig NOT identified by this algorithm as being potentially viral is dropped automatically, but we expect more false positives than false negatives\n",
    "- dc-megablast\n",
    "    - this is expected to be close to correct\n",
    "    - there may still be some false positives in this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7a584b-0b3b-4f7b-b405-cc56addaeb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if super busted, shut down kernel, remove ~/.julia, start a REPL and reinstall IJulia, then restart here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "349822c5-f469-4202-ada3-fcc561dd774f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `/global/cfs/cdirs/m4269/cjprybol/Mycelia/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `/global/cfs/cdirs/m4269/cjprybol/Mycelia/Manifest.toml`\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling Mycelia [453d265d-8292-4a7b-a57c-dce3f9ae6acd]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSkipping precompilation since __precompile__(false). Importing Mycelia [453d265d-8292-4a7b-a57c-dce3f9ae6acd].\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "KeyError: key \"usage_request\" not found",
     "output_type": "error",
     "traceback": [
      "KERNEL EXCEPTION",
      "KeyError: key \"usage_request\" not found",
      "",
      "Stacktrace:",
      " [1] getindex(h::Dict{String, Function}, key::String)",
      "   @ Base ./dict.jl:482",
      " [2] eventloop(socket::ZMQ.Socket)",
      "   @ IJulia ~/.julia/packages/IJulia/Vo51o/src/eventloop.jl:8",
      " [3] (::IJulia.var\"#14#17\")()",
      "   @ IJulia ./task.jl:417"
     ]
    }
   ],
   "source": [
    "if isfile(\"Project.toml\")\n",
    "    println(\"removing Project.toml...\")\n",
    "    rm(\"Project.toml\")\n",
    "end\n",
    "if isfile(\"Manifest.toml\")\n",
    "    println(\"removing Manifest.toml...\")\n",
    "    rm(\"Manifest.toml\")\n",
    "end\n",
    "ENV[\"LD_LIBRARY_PATH\"] = \"\"\n",
    "\n",
    "# delete baseline environment???\n",
    "\n",
    "import Pkg\n",
    "\n",
    "pkgs = [\n",
    "\"Dates\",\n",
    "\"BioSequences\",\n",
    "\"DataFrames\",\n",
    "\"FASTX\",\n",
    "\"Statistics\",\n",
    "\"StatsPlots\",\n",
    "\"uCSV\",\n",
    "\"Revise\",\n",
    "\"Kmers\",\n",
    "\"StatsBase\",\n",
    "\"ProgressMeter\",\n",
    "\"XAM\"\n",
    "]\n",
    "Pkg.add(pkgs)\n",
    "for pkg in pkgs\n",
    "    eval(Meta.parse(\"import $pkg\"))\n",
    "end\n",
    "\n",
    "import Mycelia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "909f13b4-7e9d-429b-8b83-33c99af653c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/global/cfs/cdirs/m4269/cjprybol/Mycelia/projects/viral-pangenome-discovery/data\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = joinpath(dirname(pwd()), \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbdf0fdc-def3-44bf-b0ba-58ef182cdeb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blastdbs_dir = \"$(homedir())/workspace/blastdbs\"\n",
    "taxdump_dir = mkpath(joinpath(blastdbs_dir, \"taxdump\"))\n",
    "taxdump_tar = joinpath(taxdump_dir, \"taxdump.tar.gz\")\n",
    "if !isfile(taxdump_tar)\n",
    "    run(`wget --quiet https://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz --directory-prefix=$(taxdump_dir)`)\n",
    "end\n",
    "if isempty(filter(x -> occursin(r\"\\.dmp$\", x), readdir(taxdump_dir)))\n",
    "    run(`tar -xvzf $(taxdump_tar) --directory $(taxdump_dir)`)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1edc577c-07d3-41be-9cde-412d885f7dcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/global/cfs/cdirs/m4269/cjprybol/Mycelia/projects/viral-pangenome-discovery/data/results\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "KeyError",
     "evalue": "KeyError: key \"usage_request\" not found",
     "output_type": "error",
     "traceback": [
      "KERNEL EXCEPTION",
      "KeyError: key \"usage_request\" not found",
      "",
      "Stacktrace:",
      " [1] getindex(h::Dict{String, Function}, key::String)",
      "   @ Base ./dict.jl:482",
      " [2] eventloop(socket::ZMQ.Socket)",
      "   @ IJulia ~/.julia/packages/IJulia/Vo51o/src/eventloop.jl:8",
      " [3] (::IJulia.var\"#14#17\")()",
      "   @ IJulia ./task.jl:417"
     ]
    }
   ],
   "source": [
    "SRR_paths = filter(x -> !occursin(\".ipynb_checkpoints\", x), readdir(joinpath(data_dir, \"SRA\"), join=true))\n",
    "SRR_paths = filter(x -> isfile(joinpath(x, \"megahit\", \"final.contigs.fastg.gfa.fna\")), SRR_paths)\n",
    "\n",
    "# get all taxonids at or below virus\n",
    "# mamba create -n taxonkit -c bioconda taxonkit\n",
    "# wget -c ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz \n",
    "# tar -zxvf taxdump.tar.gz\n",
    "# mkdir -p $HOME/.taxonkit\n",
    "# cp names.dmp nodes.dmp delnodes.dmp merged.dmp $HOME/.taxonkit\n",
    "# --data-dir\n",
    "viral_tax_ids = Set(parse.(Int, filter(!isempty, readlines(`mamba run -n taxonkit taxonkit list --ids 10239 --indent \"\"`))))\n",
    "n_methods = 8\n",
    "data_dir = joinpath(dirname(pwd()), \"data\")\n",
    "RESULTS_DIR = mkpath(joinpath(data_dir, \"results\"))\n",
    "\n",
    "# SRR_paths = filter(x -> !occursin(\".ipynb_checkpoints\", x), readdir(joinpath(data_dir, \"SRA\"), join=true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee1aa8c-2e78-487d-9371-a56f11be782c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "KeyError: key \"usage_request\" not found",
     "output_type": "error",
     "traceback": [
      "KERNEL EXCEPTION",
      "KeyError: key \"usage_request\" not found",
      "",
      "Stacktrace:",
      " [1] getindex(h::Dict{String, Function}, key::String)",
      "   @ Base ./dict.jl:482",
      " [2] eventloop(socket::ZMQ.Socket)",
      "   @ IJulia ~/.julia/packages/IJulia/Vo51o/src/eventloop.jl:8",
      " [3] (::IJulia.var\"#14#17\")()",
      "   @ IJulia ./task.jl:417"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   5%|██                                       |  ETA: 0:18:46\u001b[39m"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "KeyError: key \"usage_request\" not found",
     "output_type": "error",
     "traceback": [
      "KERNEL EXCEPTION",
      "KeyError: key \"usage_request\" not found",
      "",
      "Stacktrace:",
      " [1] getindex(h::Dict{String, Function}, key::String)",
      "   @ Base ./dict.jl:482",
      " [2] eventloop(socket::ZMQ.Socket)",
      "   @ IJulia ~/.julia/packages/IJulia/Vo51o/src/eventloop.jl:8",
      " [3] (::IJulia.var\"#14#17\")()",
      "   @ IJulia ./task.jl:417"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   5%|██▎                                      |  ETA: 0:22:03\u001b[39m"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "KeyError: key \"usage_request\" not found",
     "output_type": "error",
     "traceback": [
      "KERNEL EXCEPTION",
      "KeyError: key \"usage_request\" not found",
      "",
      "Stacktrace:",
      " [1] getindex(h::Dict{String, Function}, key::String)",
      "   @ Base ./dict.jl:482",
      " [2] eventloop(socket::ZMQ.Socket)",
      "   @ IJulia ~/.julia/packages/IJulia/Vo51o/src/eventloop.jl:8",
      " [3] (::IJulia.var\"#14#17\")()",
      "   @ IJulia ./task.jl:417"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   6%|██▎                                      |  ETA: 0:22:36\u001b[39m"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "KeyError: key \"usage_request\" not found",
     "output_type": "error",
     "traceback": [
      "KERNEL EXCEPTION",
      "KeyError: key \"usage_request\" not found",
      "",
      "Stacktrace:",
      " [1] getindex(h::Dict{String, Function}, key::String)",
      "   @ Base ./dict.jl:482",
      " [2] eventloop(socket::ZMQ.Socket)",
      "   @ IJulia ~/.julia/packages/IJulia/Vo51o/src/eventloop.jl:8",
      " [3] (::IJulia.var\"#14#17\")()",
      "   @ IJulia ./task.jl:417"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  11%|████▌                                    |  ETA: 0:23:27\u001b[39m"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "KeyError: key \"usage_request\" not found",
     "output_type": "error",
     "traceback": [
      "KERNEL EXCEPTION",
      "KeyError: key \"usage_request\" not found",
      "",
      "Stacktrace:",
      " [1] getindex(h::Dict{String, Function}, key::String)",
      "   @ Base ./dict.jl:482",
      " [2] eventloop(socket::ZMQ.Socket)",
      "   @ IJulia ~/.julia/packages/IJulia/Vo51o/src/eventloop.jl:8",
      " [3] (::IJulia.var\"#14#17\")()",
      "   @ IJulia ./task.jl:417"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  36%|██████████████▊                          |  ETA: 0:14:37\u001b[39m"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "KeyError: key \"usage_request\" not found",
     "output_type": "error",
     "traceback": [
      "KERNEL EXCEPTION",
      "KeyError: key \"usage_request\" not found",
      "",
      "Stacktrace:",
      " [1] getindex(h::Dict{String, Function}, key::String)",
      "   @ Base ./dict.jl:482",
      " [2] eventloop(socket::ZMQ.Socket)",
      "   @ IJulia ~/.julia/packages/IJulia/Vo51o/src/eventloop.jl:8",
      " [3] (::IJulia.var\"#14#17\")()",
      "   @ IJulia ./task.jl:417"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  84%|██████████████████████████████████▎      |  ETA: 0:04:33\u001b[39m"
     ]
    }
   ],
   "source": [
    "viral_blast_hits_joint_table = DataFrames.DataFrame()\n",
    "ProgressMeter.@showprogress for SRR_path in SRR_paths\n",
    "    # CONTIG INFO TABLE HAS JUST METADATA ABOUT THE CONTIG AND THE READS MAPPING BACK TO IT\n",
    "    contig_info_pattern = basename(SRR_path) * \".final.contigs.fastg.gfa.fna\"\n",
    "    contig_info_files = filter(x -> occursin(contig_info_pattern, x), readdir(SRR_path, join=true))\n",
    "    joint_contig_info_table = DataFrames.DataFrame()\n",
    "    for f in contig_info_files\n",
    "        table_col_types = [\n",
    "            Int64,\n",
    "            Int64,\n",
    "            Int64,\n",
    "            Float64,\n",
    "            Float64,\n",
    "            Float64,\n",
    "            String,\n",
    "            String,\n",
    "            String,\n",
    "            Int64,\n",
    "            String,\n",
    "            Float64,\n",
    "            Float64,\n",
    "            Float64,\n",
    "            Float64,\n",
    "            Int64,\n",
    "            Int64,\n",
    "            Int64,\n",
    "            Int64,\n",
    "            Int64,\n",
    "            Int64,\n",
    "            Int64,\n",
    "            Int64,\n",
    "            Int64,\n",
    "        ]\n",
    "        method = replace(replace(basename(f), basename(SRR_path) * \".final.contigs.fastg.gfa.fna.\" => \"\"), \".contig_info.tsv\" => \"\")\n",
    "        this_contig_info_table = DataFrames.DataFrame(uCSV.read(f, delim='\\t', header=1, types=table_col_types, encodings=Dict(\"\" => missing), allowmissing=true)...)\n",
    "        this_contig_info_table[!, \"Method\"] .= method\n",
    "        this_contig_info_table[!, \"SRR\"] .= basename(SRR_path)\n",
    "        append!(joint_contig_info_table, this_contig_info_table)\n",
    "    end\n",
    "    sort!(joint_contig_info_table, \"Contig\")\n",
    "    contig_info_table = unique(joint_contig_info_table[!, [\"SRR\", \"Contig\", \"Length\", \"Mapped bases\", \"Mean coverage\", \"Standard Deviation\", \"% Mapped bases\"]])\n",
    "\n",
    "    # BLAST TOP HITS TABLE HAS TOP HIT FOR EACH CONTIG FOR EACH METHOD\n",
    "    blast_classifications_table = joint_contig_info_table[.!ismissing.(joint_contig_info_table[!, \"subject id\"]), DataFrames.Not(names(contig_info_table[!, DataFrames.Not(\"Contig\")]))]\n",
    "    sort!(blast_classifications_table, [\"Contig\", \"evalue\"])\n",
    "    blast_hits_top_hits_table = DataFrames.combine(DataFrames.groupby(blast_classifications_table, [\"Contig\", \"Method\"]), first)\n",
    "    blast_hits_top_hits_table = blast_hits_top_hits_table[map(x -> x in viral_tax_ids, blast_hits_top_hits_table[!, \"subject tax id\"]), :]\n",
    "\n",
    "    # JOINT LCA TABLE HAS CLASSIFICATION FOR EACH MMSEQS RUN\n",
    "    joint_lca_table = DataFrames.DataFrame()\n",
    "    easy_taxonomy_lca_reports = filter(x -> occursin(\"final.contigs.fastg.gfa.fna.mmseqs_easy_taxonomy.\", x) && occursin(\"_lca.tsv\", x), readdir(joinpath(SRR_path, \"mmseqs_easy_taxonomy\"), join=true))\n",
    "    for lca_tsv in easy_taxonomy_lca_reports\n",
    "        method = replace(replace(basename(lca_tsv), \"final.contigs.fastg.gfa.fna.\" => \"\"), \"_lca.tsv\" => \"\")\n",
    "        this_lca_table = Mycelia.parse_mmseqs_easy_taxonomy_lca_tsv(lca_tsv)\n",
    "        this_lca_table[!, \"SRR\"] .= basename(SRR_path)\n",
    "        this_lca_table[!, \"SRR\"] .= method\n",
    "        append!(joint_lca_table, this_lca_table)\n",
    "    end\n",
    "    sort!(joint_lca_table, \"contig_id\")\n",
    "    joint_lca_table = joint_lca_table[map(x -> x in viral_tax_ids, joint_lca_table[!, \"taxon_id\"]), :]\n",
    "\n",
    "    # JUST VIRSORTER\n",
    "    virsorter_score_tsv = joinpath(SRR_path, \"virsorter\", \"final-viral-score.tsv\")\n",
    "    virsorter_results = Mycelia.parse_virsorter_score_tsv(virsorter_score_tsv)\n",
    "    virsorter_results[!, \"seqname\"] = parse.(Int, first.(split.(virsorter_results[!, \"seqname\"], '|')))\n",
    "\n",
    "    # JUST GENOMAD\n",
    "    genomad_virus_summary = joinpath(SRR_path, \"genomad\", \"final.contigs.fastg.gfa_summary\", \"final.contigs.fastg.gfa_virus_summary.tsv\")\n",
    "    genomad_results = DataFrames.DataFrame(uCSV.read(genomad_virus_summary, delim='\\t', header=1, typedetectrows=100)...)\n",
    "\n",
    "    number_of_hits = StatsBase.countmap(vcat(\n",
    "        blast_hits_top_hits_table[!, \"Crontig\"],\n",
    "        joint_lca_table[!, \"contig_id\"],\n",
    "        virsorter_results[!, \"seqname\"],\n",
    "        genomad_results[!, \"seq_name\"]\n",
    "    ))\n",
    "\n",
    "    majority_support_contigs = Set(keys(filter(x -> x[2] >= (n_methods/2), number_of_hits)))\n",
    "    viral_blast_top_hits_table = blast_hits_top_hits_table[map(x -> x in majority_support_contigs, blast_hits_top_hits_table[!, \"Contig\"]), :]\n",
    "    viral_blast_top_hits_table = viral_blast_top_hits_table[!, [\"% identity\", \"alignment length\"]]\n",
    "    append!(viral_blast_hits_joint_table, viral_blast_top_hits_table)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1fa425-76be-46c0-b6af-45ed82cfec96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = StatsPlots.scatter(\n",
    "    viral_blast_hits_joint_table[!, \"alignment length\"],\n",
    "    viral_blast_hits_joint_table[!, \"% identity\"],\n",
    "    title = \"aligment quality for conensus viral contigs\",\n",
    "    xlabel = \"alignment length\",\n",
    "    ylabel = \"% identity\",\n",
    "    legend=false\n",
    ")\n",
    "\n",
    "now_string = replace(string(Dates.now()), r\"[^\\d+]\" => \"\")\n",
    "StatsPlots.savefig(p, joinpath(RESULTS_DIR, \"figure-1.$(now_string).png\"))\n",
    "display(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7605085-192e-4c3f-a336-45ee7becc498",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = StatsPlots.scatter(\n",
    "    viral_blast_hits_joint_table[!, \"alignment length\"],\n",
    "    viral_blast_hits_joint_table[!, \"% identity\"],\n",
    "    title = \"aligment quality for conensus viral contigs\",\n",
    "    xlabel = \"alignment length\",\n",
    "    ylabel = \"% identity\",\n",
    "    legend=false\n",
    ")\n",
    "\n",
    "now_string = replace(string(Dates.now()), r\"[^\\d+]\" => \"\")\n",
    "StatsPlots.savefig(p, joinpath(RESULTS_DIR, \"figure-1.$(now_string).png\"))\n",
    "display(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef313740-89bc-4789-9ab7-9652914e2bfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# contig_info_table[!, \"viral_classification_count\"] = map(contig -> get(contig_support_counts, contig, 0), contig_info_table[!, \"Contig\"])\n",
    "# contig_info_table[!, \"viral_classification_percent\"] = round.((contig_info_table[!, \"viral_classification_count\"] ./ n_methods) .* 100, digits=1)\n",
    "# end\n",
    "# sample_summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ce2b12-102a-40c7-931d-cac3e666daa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SRR_paths\n",
    "# SRR_path = rand(SRR_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aebdec-a439-460c-8228-69e2dfed95f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IMGVR_sam = joinpath(SRR_path, \"megahit\", \"final.contigs.fastg.gfa.viral.fna.IMGVR_all_nucleotides-high_confidence.fna.gz.sam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e85679-4efb-4a12-8d60-2fd923503a48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# uCSV.read(IMGVR_sam, delim='\\t', typedetectrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d54aa2-e91f-4ba7-8bd6-da27493da712",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reader = open(XAM.SAM.Reader, IMGVR_sam)\n",
    "# # Iterate over BAM records.\n",
    "# for record in reader\n",
    "#     # `record` is a BAM.Record object.\n",
    "#     if XAM.SAM.ismapped(record)\n",
    "#         # Print the mapped position.\n",
    "#         println(XAM.SAM.refname(record), ':', XAM.SAM.position(record))\n",
    "#     end\n",
    "# end\n",
    "\n",
    "# # Close the BAM file.\n",
    "# close(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6655bbe-5035-4621-89ba-4335dba973fa",
   "metadata": {},
   "source": [
    "Based on these results, we will go for a 3x confirmation via targetted blast where the sequence must have a high confidence match for a viral hit using:\n",
    "- ref_viruses_rep_genomes_blastn\n",
    "- ref_viruses_rep_genomes_dcmegablast\n",
    "- nt_viral_validation_megablast\n",
    "\n",
    "Full nt database attempts to validate viral contigs came back primarily with bacterial artificial chromosomes associated with human cell lines that did not seem like valuable hits to us. We also didn't expect to find *novel* viruses by megablasting against the ref_viruses_rep_genomes, given that it is a limited, representative set of all known viruses and the official description of the algorithm is \"Traditional megablast used to find very similar (e.g., intraspecies or closely related species) sequences\"\n",
    "https://www.ncbi.nlm.nih.gov/books/NBK569839/\n",
    "\n",
    "Because of this, we felt that getting a multiple redudant hits using the most flexibile algorithm (blastn) against the highest quality blast db (ref_viruses_rep_genomes), a less flexible, but still cross-species algorithm (dc-megablast: Discontiguous megablast used to find more distant (e.g., interspecies) sequences), and finally a 3rd validation hit against a potentially lower quality due to less manual curation viral database (nt_viral) using the strictest alogrithm, megablast.\n",
    "\n",
    "Because of the low concordance of Kraken classifications to blast classifications, we consider the calls with much less weight than the classified contigs, but include them because the data was generated and it may prove informative to others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6686068b-a98d-4a1e-9719-c1891057396b",
   "metadata": {},
   "source": [
    "As an update to the above, we found that we were unable to perform the `nt_viral_validation_megablast` in a reasonable amount of time, so we will be dropping that requirement.\n",
    "\n",
    "Instead, we will perform a final re-assembly and then perform the final validation blast and protein-level classification on that final assembly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3da95f-d7b7-494f-8022-960a9727b241",
   "metadata": {},
   "source": [
    "The following was a manual, indepth analysis of the first sample that we used to inform our validation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2e4b7f-56b3-4ea3-9f93-a2541f067dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract contigs that came back as viral in the targetted screen\n",
    "# # blast back against NCBI nt and confirm they are still viral\n",
    "# SRR_path = first(SRR_paths)\n",
    "# SRR = basename(SRR_path)\n",
    "\n",
    "# # ref_viruses_ref_genomes_blast_report = joinpath(SRR_path, \"blastn\", \"final.contigs.fastg.gfa.fna.blastn.ref_viruses_rep_genomes.blastn.txt\")\n",
    "# # ref_viruses_ref_genomes_blast_results = Mycelia.parse_blast_report(ref_viruses_ref_genomes_blast_report)\n",
    "# # possible_viral_contigs = Set(unique(ref_viruses_ref_genomes_blast_results[!, \"query id\"]))\n",
    "# # assembled_fasta = joinpath(SRR_path, \"megahit\", \"final.contigs.fastg.gfa.fna\")\n",
    "# # viral_fasta = replace(assembled_fasta, \".fna\" => \".potential_viral_contigs.fna\")\n",
    "# # potential_viral_records = filter(x -> FASTX.identifier(x) in possible_viral_contigs, collect(Mycelia.open_fastx(assembled_fasta)))\n",
    "# # open(viral_fasta, \"w\") do io\n",
    "# #     fastx_io = FASTX.FASTA.Writer(io)\n",
    "# #     for record in potential_viral_records\n",
    "# #         write(fastx_io, record)\n",
    "# #     end\n",
    "# #     close(fastx_io)\n",
    "# # end\n",
    "\n",
    "# # potential_viral_records\n",
    "\n",
    "\n",
    "# taxon_id_to_kingdom_map = Dict{Int, String}()\n",
    "# kingdom_to_taxon_id_map = Dict(\n",
    "#     \"Viruses\" => 10239,\n",
    "#     \"Archaea\" => 2157,\n",
    "#     \"Bacteria\" => 2,\n",
    "#     \"Eukaryota\" => 2759,\n",
    "#     \"Other\" => 28384,\n",
    "#     \"Unclassified\" => 12908\n",
    "# )\n",
    "# ProgressMeter.@showprogress for (kingdom, taxon_id) in kingdom_to_taxon_id_map\n",
    "#     for child_taxon_id in parse.(Int, filter(!isempty, readlines(`taxonkit list --data-dir $(taxdump_dir) --ids $(taxon_id) --indent=\"\"`)))\n",
    "#         taxon_id_to_kingdom_map[child_taxon_id] = kingdom\n",
    "#     end\n",
    "# end\n",
    "\n",
    "# ref_viruses_rep_genomes_blastn_contigs = unique(ref_viruses_rep_genomes_blastn_results[!, \"query id\"])\n",
    "\n",
    "# ref_viruses_rep_genomes_dcmegablast_contigs = unique(ref_viruses_rep_genomes_dcmegablast_results[!, \"query id\"])\n",
    "\n",
    "# # # 8 non-overlapping hits!\n",
    "# # union(ref_viruses_rep_genomes_dcmegablast_contigs, ref_viruses_rep_genomes_blastn_contigs)\n",
    "# # # these all have pretty low e-values, I'm not going to worry about them\n",
    "# # dcmegablast_only = setdiff(ref_viruses_rep_genomes_dcmegablast_contigs, ref_viruses_rep_genomes_blastn_contigs)\n",
    "# # ref_viruses_rep_genomes_dcmegablast_results[map(x -> x in dcmegablast_only, ref_viruses_rep_genomes_dcmegablast_results[!, \"query id\"]), :]\n",
    "\n",
    "# ref_viruses_rep_genomes_megablast_contigs = unique(ref_viruses_rep_genomes_megablast_results[!, \"query id\"])\n",
    "# # nt_viral_validation_megablast_results\n",
    "# # nt_validation_megablast_results\n",
    "# # all megablast hits are subset of blastn hits\n",
    "# # union(ref_viruses_rep_genomes_megablast_contigs, ref_viruses_rep_genomes_blastn_contigs)\n",
    "\n",
    "# nt_viral_validation_megablast_contigs = unique(nt_viral_validation_megablast_results[!, \"query id\"])\n",
    "# # nt_validation_megablast_results\n",
    "\n",
    "# # union(nt_viral_validation_megablast_contigs, ref_viruses_rep_genomes_blastn_contigs)\n",
    "# # map(x -> x in viral_taxon_ids, nt_viral_validation_megablast_results[!, \"subject tax id\"])\n",
    "\n",
    "# taxon_id_to_kingdom_map\n",
    "\n",
    "# # only 12 contigs are still considered viral contigs after mapping to nt complete, but the hits aren't very convicing (bacterial artificial chromosomes?)\n",
    "# nt_validation_megablast_contigs = unique(nt_validation_megablast_results[map(x -> get(taxon_id_to_kingdom_map, x, \"\") == \"Viruses\", nt_validation_megablast_results[!, \"subject tax id\"]), \"query id\"])\n",
    "\n",
    "# full_contig_set = union(nt_validation_megablast_contigs, nt_viral_validation_megablast_contigs, ref_viruses_rep_genomes_blastn_contigs, ref_viruses_rep_genomes_dcmegablast_contigs, ref_viruses_rep_genomes_megablast_contigs)\n",
    "\n",
    "# # get all of the reads mapping to each\n",
    "\n",
    "# bam_file = joinpath(SRR_path, \"megahit\", \"final.contigs.fastg.gfa.fna.bwa.bam\")\n",
    "# # bamfile = first(filter(x -> occursin(r\"\\.bam$\", x), readdir(joinpath(SRR_dir, \"megahit\"), join=true)))\n",
    "\n",
    "# # implement as the following nested dictionary\n",
    "# # contigs => reads => taxon_id\n",
    "\n",
    "# function generate_contig_to_reads_map(bamfile, contigs_of_interest)\n",
    "#     contigs_to_reads_map = Dict(contig => Set{String}() for contig in contigs_of_interest)\n",
    "#     # reads_of_interest = Set{String}()\n",
    "#     reader = open(XAM.BAM.Reader, bamfile)\n",
    "#     for record in reader\n",
    "#         if XAM.BAM.ismapped(record) && (XAM.BAM.refname(record) in contigs_of_interest)\n",
    "#             push!(contigs_to_reads_map[XAM.BAM.refname(record)], XAM.BAM.tempname(record))\n",
    "#         end\n",
    "#     end\n",
    "#     close(reader)\n",
    "#     return contigs_to_reads_map\n",
    "# end\n",
    "\n",
    "# # 1200 seconds\n",
    "# @time contigs_to_reads_map = generate_contig_to_reads_map(bam_file, full_contig_set)\n",
    "\n",
    "# function read_kraken_output(kraken_output)\n",
    "#     # read_kraken_report\n",
    "#     header = [\n",
    "#         \"classification status\",\n",
    "#         \"sequence ID\",\n",
    "#         \"taxon ID\",\n",
    "#         \"sequence length\",\n",
    "#         \"LCA mappings\"\n",
    "#     ]\n",
    "#     data, _ = uCSV.read(IOBuffer(join(filtered_lines, '\\n')), delim='\\t')\n",
    "#     return DataFrames.DataFrame(data, header)\n",
    "# end\n",
    "\n",
    "# # readdir(joinpath(SRR_path, \"kraken\"))\n",
    "\n",
    "# reads_of_interest = reduce(union, values(contigs_to_reads_map))\n",
    "\n",
    "# kraken_output = last(filter(x -> occursin(r\"\\.kraken-output\\.tsv\", x), readdir(joinpath(SRR_path, \"kraken\"), join=true)))\n",
    "# if occursin(r\"\\.gz\", kraken_output)\n",
    "#     kraken_buffer = open(`gzip -dc $(kraken_output)`)\n",
    "# else\n",
    "#     kraken_buffer = open(kraken_output)\n",
    "# end\n",
    "\n",
    "# filtered_lines = String[]\n",
    "# for line in eachline(kraken_buffer)\n",
    "#     split_line = split(line, '\\t')\n",
    "#     if split_line[2] in reads_of_interest\n",
    "#         push!(filtered_lines, line)\n",
    "#     end\n",
    "# end\n",
    "# close(kraken_buffer)\n",
    "\n",
    "# read_classifications = read_kraken_output(IOBuffer(join(filtered_lines, '\\n')))\n",
    "# read_classifications[!, \"parsed taxon ID\"] = map(x -> match(r\"\\(taxid (\\d+)\\)\", x).captures[1], read_classifications[!, \"taxon ID\"])\n",
    "# read_classifications\n",
    "\n",
    "# read_classifications_map = Dict(row[\"sequence ID\"] => parse(Int, row[\"parsed taxon ID\"]) for row in DataFrames.eachrow(read_classifications))\n",
    "\n",
    "# contigs_to_taxon_counts_map = Dict()\n",
    "# for (contig, reads) in contigs_to_reads_map\n",
    "#     contigs_to_taxon_counts_map[contig] = StatsBase.countmap(get(taxon_id_to_kingdom_map, read_classifications_map[read], \"Unclassified\") for read in reads)\n",
    "# end\n",
    "# contigs_to_taxon_counts_map\n",
    "\n",
    "# contigs_to_taxon_proportions_map = Dict()\n",
    "# for (contig, taxon_counts) in contigs_to_taxon_counts_map\n",
    "#     total_count = sum(values(taxon_counts))\n",
    "#     contigs_to_taxon_proportions_map[contig] = Dict(kingdom => count / total_count for (kingdom, count) in taxon_counts)\n",
    "# end\n",
    "# contigs_to_taxon_proportions_map\n",
    "\n",
    "# # ref_viruses_rep_genomes_blastn_results\n",
    "# # ref_viruses_rep_genomes_dcmegablast_results\n",
    "# # ref_viruses_rep_genomes_megablast_results\n",
    "# # nt_viral_validation_megablast_results\n",
    "# # nt_validation_megablast_results\n",
    "\n",
    "# contig_classification_results = \n",
    "# DataFrames.DataFrame(\n",
    "#     union(\n",
    "#         DataStructures.OrderedDict(\"Contig\" => String[]),\n",
    "#         DataStructures.OrderedDict(k => Float64[] for k in keys(kingdom_to_taxon_id_map)),\n",
    "#         # these are ordered by most hits to fewest hits\n",
    "#         DataStructures.OrderedDict(db_algorithm => Bool[] for db_algorithm in [\"ref_viruses_rep_genomes_blastn\", \"ref_viruses_rep_genomes_dcmegablast\", \"nt_viral_validation_megablast\", \"ref_viruses_rep_genomes_megablast\", \"nt_validation_megablast\"])\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# for (contig, taxon_proportions) in contigs_to_taxon_proportions_map\n",
    "#     row = Dict{Any, Any}(\"Contig\" => contig)\n",
    "#     for k in keys(kingdom_to_taxon_id_map)\n",
    "#         row[k] = get(taxon_proportions, k, 0.0)\n",
    "#     end\n",
    "#     row[\"ref_viruses_rep_genomes_blastn\"] = contig in ref_viruses_rep_genomes_blastn_contigs\n",
    "#     row[\"nt_viral_validation_megablast\"] = contig in nt_viral_validation_megablast_contigs\n",
    "#     row[\"ref_viruses_rep_genomes_dcmegablast\"] = contig in ref_viruses_rep_genomes_dcmegablast_contigs\n",
    "#     row[\"ref_viruses_rep_genomes_megablast\"] = contig in ref_viruses_rep_genomes_megablast_contigs\n",
    "#     row[\"nt_validation_megablast\"] = contig in nt_validation_megablast_contigs\n",
    "#     push!(contig_classification_results, row)\n",
    "# end\n",
    "# contig_classification_results[!, \"top_kingdom\"] .= \"\"\n",
    "# for (i, row) in enumerate(DataFrames.eachrow(contig_classification_results))\n",
    "#     max_hit = \"\"\n",
    "#     max_value = 0.0\n",
    "#     for k in keys(kingdom_to_taxon_id_map)\n",
    "#         if row[k] > max_value\n",
    "#             max_value = row[k]\n",
    "#             max_hit = k\n",
    "#         end\n",
    "#     end\n",
    "#     contig_classification_results[i, \"top_kingdom\"] = max_hit\n",
    "# end\n",
    "\n",
    "# m = Int.(Matrix(\n",
    "#     contig_classification_results[!, [\n",
    "#         \"ref_viruses_rep_genomes_blastn\",\n",
    "#         \"ref_viruses_rep_genomes_dcmegablast\",\n",
    "#         \"nt_viral_validation_megablast\",\n",
    "#         \"ref_viruses_rep_genomes_megablast\",\n",
    "#         \"nt_validation_megablast\"\n",
    "#     ]\n",
    "# ]))\n",
    "\n",
    "# contig_classification_results[!, \"blast_hits\"] = map(r -> sum(r), eachrow(m))\n",
    "\n",
    "# contig_classification_results\n",
    "\n",
    "# # eukaryotic top hit - kraken is asserting these are human contamination\n",
    "# sum(contig_classification_results[!, \"Eukaryota\"])\n",
    "\n",
    "# # novel sequences are second hit, this is exciting!\n",
    "# sum(contig_classification_results[!, \"Unclassified\"])\n",
    "\n",
    "# sum(contig_classification_results[!, \"Bacteria\"])\n",
    "\n",
    "# sum(contig_classification_results[!, \"Other\"])\n",
    "\n",
    "# sum(contig_classification_results[!, \"Viruses\"])\n",
    "\n",
    "# sum(contig_classification_results[!, \"Archaea\"])\n",
    "\n",
    "# contig_classification_results_summary = contig_classification_results[!, [\"Contig\", \"top_kingdom\", \"blast_hits\"]]\n",
    "\n",
    "# contig_classification_results_summary\n",
    "\n",
    "# StatsBase.countmap(contig_classification_results_summary[!, \"blast_hits\"])\n",
    "\n",
    "# contig_classification_results_summary[contig_classification_results_summary[!, \"blast_hits\"] .== 5, :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.7",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
