{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce59cd77-c0f0-4b90-8de8-3c5f071935ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.activate(\".\")\n",
    "\n",
    "# not for the faint of heart!\n",
    "# Pkg.update()\n",
    "\n",
    "pkgs = [\n",
    "\"ArgParse\",\n",
    "\"Base64\",\n",
    "\"BioSequences\",\n",
    "\"DataFrames\",\n",
    "\"Dates\",\n",
    "\"DelimitedFiles\",\n",
    "\"FASTX\",\n",
    "\"GLM\",\n",
    "\"HTTP\",\n",
    "\"JSON\",\n",
    "\"Graphs\",\n",
    "\"MetaGraphs\",\n",
    "\"MD5\",\n",
    "\"Statistics\",\n",
    "\"StatsPlots\",\n",
    "\"uCSV\",\n",
    "\"CodecZlib\",\n",
    "\"YAML\",\n",
    "\"Revise\",\n",
    "\"Kmers\",\n",
    "\"StatsBase\",\n",
    "\"ProgressMeter\"\n",
    "]\n",
    "Pkg.add(pkgs)\n",
    "for pkg in pkgs\n",
    "    eval(Meta.parse(\"import $pkg\"))\n",
    "end\n",
    "\n",
    "import Mycelia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909f13b4-7e9d-429b-8b83-33c99af653c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = joinpath(dirname(pwd()), \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4546da-d041-4b4e-9754-abade6bd574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create full refseq database with taxonomic IDs from scratch\n",
    "\n",
    "# blastdb_dir = mkpath(\"$(homedir())/workspace/blastdbs/refseq\")\n",
    "\n",
    "# refseq_metadata = Mycelia.load_refseq_metadata();\n",
    "\n",
    "# gcf_to_url_map = Dict(row[\"# assembly_accession\"] => Mycelia.ncbi_ftp_path_to_url(ftp_path = row[\"ftp_path\"], extension = \"genomic.fna.gz\") for row in DataFrames.eachrow(refseq_metadata))\n",
    "\n",
    "# expected_list_of_fasta_files = basename.(values(gcf_to_url_map))\n",
    "# actual_list_of_fasta_files = filter(x -> occursin(r\"genomic\\.fna\\.gz$\", x), readdir(blastdb_dir))\n",
    "# fasta_files_to_remove = setdiff(actual_list_of_fasta_files, expected_list_of_fasta_files)\n",
    "\n",
    "# for f in fasta_files_to_remove\n",
    "#     rm(joinpath(blastdb_dir, f))\n",
    "# end\n",
    "\n",
    "# missing_fasta_files = setdiff(expected_list_of_fasta_files, actual_list_of_fasta_files)\n",
    "\n",
    "# # using ProgressMeter\n",
    "# # p = Progress(10)\n",
    "# # Threads.@threads for i in 1:10\n",
    "# #     sleep(2*rand())\n",
    "# #     next!(p)\n",
    "# # end\n",
    "# # finish!(p)\n",
    "\n",
    "# ProgressMeter.@showprogress for f in missing_fasta_files\n",
    "#     GCF_identifier = join(split(f, '_')[1:2], '_')\n",
    "#     url = gcf_to_url_map[GCF_identifier]\n",
    "#     out_f = joinpath(blastdb_dir, f)\n",
    "#     # @show f, out_f\n",
    "#     if !isfile(out_f)\n",
    "#         download(url, out_f)\n",
    "#     end\n",
    "# end\n",
    "\n",
    "# actual_list_of_fasta_files = filter(x -> occursin(r\"genomic\\.fna\\.gz$\", x), readdir(blastdb_dir))\n",
    "# # @assert actual_list_of_fasta_files == expected_list_of_fasta_files\n",
    "# list_of_fasta_files = intersect(actual_list_of_fasta_files, expected_list_of_fasta_files)\n",
    "\n",
    "# ProgressMeter.@showprogress for f in list_of_fasta_files\n",
    "#     f_path = joinpath(blastdb_dir, f)\n",
    "#     headers_file = f_path * \".headers.txt\"\n",
    "#     if !isfile(headers_file)\n",
    "#         p1 = pipeline(`gzip -dc $(f_path)`, `grep \"^>\"`)\n",
    "#         p2 = pipeline(p1, headers_file)\n",
    "#         run(p2)\n",
    "#     end\n",
    "# end\n",
    "\n",
    "# # find -name \"*_genomic.fna.gz\" > genome_list.txt\n",
    "# # head -n3 genome_list.txt > genome_list.3.txt\n",
    "\n",
    "# # $ parallel --shellquote\n",
    "# # parallel: Warning: Input is read from the terminal. You either know what you\n",
    "# # parallel: Warning: are doing (in which case: YOU ARE AWESOME!) or you forgot\n",
    "# # parallel: Warning: ::: or :::: or to pipe data into parallel. If so\n",
    "# # parallel: Warning: consider going through the tutorial: man parallel_tutorial\n",
    "# # parallel: Warning: Press CTRL-D to exit.\n",
    "# # gzip -dc {} | grep \"^>\" > {}.headers.txt\n",
    "# # parallel gzip\\ -dc\\ \\{\\}\\ \\|\\ grep\\ \\\"\\^\\>\\\"\\ \\>\\ \\{\\}.headers.txt :::: genome_list.3.txt\n",
    "\n",
    "# # parallel gzip\\ -dc\\ \\{\\}\\ \\|\\ grep\\ \\\"\\^\\>\\\"\\ \\>\\ \\{\\}.headers.txt :::: genome_list.txt\n",
    "\n",
    "# # NOTE instead of making a giant joint fasta, it seems possible to make a db for each fasta, and then link them\n",
    "# # this would be more efficient to update and revise on the fly\n",
    "# # would also probably be more storage efficient\n",
    "# # will try both\n",
    "\n",
    "# # blastdb_aliastool -dblist \"database_1 database_2 and so on\" -dbtype nucl -out your_linked_db_name -title \"My full DB\"\n",
    "# # Late to the conversation but I realized something important.\n",
    "# # The above may not work if your concatenation results in a fasta file larger than \"1000000000\" bytes. The makeblastdb output specifies this number.\n",
    "# # The safer option is to build a database from each fasta file (after verifying that the file is less than 1000000000B and splitting if necessary; latter can be done like this: https://biopython.org/wiki/Split_large_file).\n",
    "# # Then you can use the blastdb_aliastool like so (adapted from NCBI's manual):\n",
    "# # blastdb_aliastool -dblist \"database_1 database_2 and so on\" -dbtype nucl -out your_linked_db_name -title \"My full DB\"\n",
    "\n",
    "# # NOTE, CAN PROBABLY SKIP MAKING A JOINT FILE\n",
    "# joint_fasta_file = joinpath(blastdb_dir, \"refseq.fna.gz\")\n",
    "# if !isfile(joint_fasta_file)\n",
    "#     # 39311.386470 seconds\n",
    "#     # 10 hours!!\n",
    "#     @time Mycelia.merge_fasta_files(joinpath.(blastdb_dir, list_of_fasta_files), joint_fasta_file)\n",
    "# else\n",
    "#     @info \"$(joint_fasta_file) present, remove to regenerate...\"\n",
    "# end\n",
    "\n",
    "# fasta_to_taxon_id_map = gcf_to_url_map = Dict(basename(Mycelia.ncbi_ftp_path_to_url(ftp_path = row[\"ftp_path\"], extension = \"genomic.fna.gz\")) => row[\"taxid\"] for row in DataFrames.eachrow(refseq_metadata))\n",
    "\n",
    "#  # -taxid_map <File_In>\n",
    "#  #   Text file mapping sequence IDs to taxonomy IDs.\n",
    "#  #   Format:<SequenceId> <TaxonomyId><newline>\n",
    "#  #    * Requires:  parse_seqids\n",
    "#  #    * Incompatible with:  taxid\n",
    "\n",
    "# # Progress: 100%|█████████████████████████████████████████| Time: 1:16:56\n",
    "# taxon_map_file = joint_fasta_file * \".taxon_map.txt\"\n",
    "# if !isfile(taxon_map_file)\n",
    "#     open(taxon_map_file, \"w\") do io\n",
    "#         ProgressMeter.@showprogress for f in list_of_fasta_files\n",
    "#             f_path = joinpath(blastdb_dir, f)\n",
    "#             headers_file = f_path * \".headers.txt\"\n",
    "#             taxon_id = fasta_to_taxon_id_map[f]\n",
    "#             for line in eachline(headers_file)\n",
    "#                 sequence_id = replace(first(split(line)), \">\" => \"\")\n",
    "#                 println(io, \"$(sequence_id) $(taxon_id)\")\n",
    "#             end\n",
    "#         end\n",
    "#     end\n",
    "# else\n",
    "#     @info \"$(taxon_map_file) already present...\"\n",
    "# end\n",
    "\n",
    "# # consider trying these to improve performance later???\n",
    "# # @time run(`dustmasker -in $(decompressed_joint_fasta_file) -infmt fasta -parse_seqids -outfmt maskinfo_asn1_bin -out $(decompressed_joint_fasta_file).asnb`)\n",
    "# # dustmasker -in hs_chr -infmt blastdb -parse_seqids -outfmt maskinfo_asn1_bin -out hs_chr_dust.asnb\n",
    "# # makeblastdb -in hs_chr –input_type blastdb -dbtype nucl -parse_seqids -mask_data hs_chr_mask.asnb -out hs_chr -title \"Human Chromosome, Ref B37.1\"\n",
    "\n",
    "# db_title = \"Refseq $(Dates.today())\"\n",
    "\n",
    "# # refseq_db = decompressed_joint_fasta_file\n",
    "\n",
    "# # start time Building a new DB, current time: 03/16/2023 16:13:05\n",
    "# # end time ???\n",
    "\n",
    "# # (base) 2023-03-17T21:11:40 jovyan@76eefed4ebe2:~/.../blastdbs/refseq\n",
    "# # $ time gzip -dc refseq.fna.gz | makeblastdb -in - -dbtype nucl -parse_seqids -out refseq.fna.gz -title \"Refseq 2023-03-17\" -taxid_map refseq.fna.gz.taxon_map.txt\n",
    "\n",
    "\n",
    "# # Building a new DB, current time: 03/17/2023 21:11:44\n",
    "# # New DB name:   /home/jovyan/work/blastdbs/refseq/refseq.fna.gz\n",
    "# # New DB title:  Refseq 2023-03-17\n",
    "# # Sequence type: Nucleotide\n",
    "# # Deleted existing Nucleotide BLAST database named /home/jovyan/work/blastdbs/refseq/refseq.fna.gz\n",
    "# # Keep MBits: T\n",
    "# # Maximum file size: 3000000000B\n",
    "# # Adding sequences from FASTA; added 42462451 sequences in 42543.2 seconds.\n",
    "\n",
    "\n",
    "\n",
    "# # real    717m46.778s\n",
    "# # user    761m57.365s\n",
    "# # sys     51m2.045s\n",
    "\n",
    "\n",
    "# # p = pipeline(`gzip -dc $(joint_fasta_file)`, `makeblastdb -in - -dbtype nucl -parse_seqids -out $(joint_fasta_file) -title $(db_title) -taxid_map $(taxon_map_file)`)\n",
    "# # @time run(p)\n",
    "# # @time run(`makeblastdb -in $(decompressed_joint_fasta_file) -dbtype nucl -parse_seqids -out $(decompressed_joint_fasta_file) -title $(db_title) -taxid_map $(taxon_map_file)`)\n",
    "\n",
    "\n",
    "# # gzip -dc refseq.fna.gz | makeblastdb -in - -dbtype nucl -parse_seqids -out refseq.fna.gz -title \"Refseq 2023-03-17\" -taxid_map refseq.fna.gz.taxon_map.txt\n",
    "\n",
    "# # ll -tr refseq*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7a504f-4f10-47ee-a6b5-6d32ccf71d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# windowmasker and dustmasker - not currently using\n",
    "\n",
    "# window_masker_step_1_outfile = \"$(joint_fasta_file).window_masker.1\"\n",
    "# @time run(`windowmasker -mk_counts -infmt fasta -in $(joint_fasta_file) -out $(window_masker_step_1_outfile)`)\n",
    "# window_masker_step_2_outfile = \"$(joint_fasta_file).window_masker.2\"\n",
    "# @time run(`windowmasker -ustat $(window_masker_step_1_outfile) -dust true -in $(joint_fasta_file) -infmt fasta -out $(window_masker_step_2_outfile)`)\n",
    "\n",
    "# run(`makeblastdb -in nt –input_type blastdb -dbtype nucl -parse_seqids -mask_data nt.wm.2 -out nt_masked_deduped -title \"nt masked and deduped\"`)\n",
    "\n",
    "    # windowmasker -mk_counts -checkdup true -infmt blastdb -in nt -out nt.wm.1\n",
    "    # windowmasker -ustat nt.wm.1 -dust true -in nt -infmt blastdb -out nt.wm.2\n",
    "    # makeblastdb -in nt –input_type blastdb -dbtype nucl -parse_seqids -mask_data nt.wm.2 -out nt_masked_deduped -title \"nt masked and deduped\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf0fdc-def3-44bf-b0ba-58ef182cdeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "blastdbs_dir = \"$(homedir())/workspace/blastdbs\"\n",
    "\n",
    "# # most generous\n",
    "# blast_db = \"ref_viruses_rep_genomes\"\n",
    "# blastn_task = \"blastn\"\n",
    "\n",
    "# less generous\n",
    "blast_db = \"ref_viruses_rep_genomes\"\n",
    "blastn_task = \"dc-megablast\"\n",
    "\n",
    "# # validation\n",
    "# blast_db = \"nt_viruses\"\n",
    "# blastn_task = \"dc-megablast\"\n",
    "\n",
    "blastdb_dir = mkpath(joinpath(blastdbs_dir, blast_db))\n",
    "if !isempty(readdir(blastdb_dir))\n",
    "    @info \"blast db detected, using existing\"\n",
    "else\n",
    "    Mycelia.download_blast_db(db=blast_db, outdir=blastdb_dir, source=\"ncbi\")\n",
    "end\n",
    "\n",
    "taxdump_dir = mkpath(joinpath(blastdbs_dir, \"taxdump\"))\n",
    "taxdump_tar = joinpath(taxdump_dir, \"taxdump.tar.gz\")\n",
    "if !isfile(taxdump_tar)\n",
    "    run(`wget --quiet https://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz --directory-prefix=$(taxdump_dir)`)\n",
    "end\n",
    "if isempty(filter(x -> occursin(r\"\\.dmp$\", x), readdir(taxdump_dir)))\n",
    "    run(`tar -xvzf $(taxdump_tar) --directory $(taxdump_dir)`)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9551555-94c9-4c63-b1b1-eb3bcd4ac087",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = joinpath(blastdb_dir, blast_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5685b842-daff-4e86-9235-4b8094979ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRR_paths = filter(x -> !occursin(\".ipynb_checkpoints\", x), readdir(joinpath(data_dir, \"SRA\"), join=true))\n",
    "SRR_paths = filter(x -> isfile(joinpath(x, \"megahit\", \"final.contigs.fastg.gfa.fna\")), SRR_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34524a10-9dae-416a-bd9c-0837988ea4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for SRR_path in SRR_paths\n",
    "    SRR = basename(SRR_path)\n",
    "    # @show SRR\n",
    "    contig_info_tsv = \"$(SRR_path)/$(SRR).final.contigs.fastg.gfa.fna.blastn.$(blast_db).$(blastn_task).contig_info.tsv\"\n",
    "    if isfile(contig_info_tsv)\n",
    "        # @info \"$(contig_info_tsv) already present, skipping...\"\n",
    "        continue\n",
    "    end\n",
    "\n",
    "    assembled_fasta = joinpath(SRR_path, \"megahit\", \"final.contigs.fastg.gfa.fna\")\n",
    "\n",
    "    # # run megablast to get first quick pass at species-level identifications\n",
    "    # # 12 seconds\n",
    "    # @time megablast_report = Mycelia.run_blastn(out_dir = SRR_path, fasta = assembled_fasta, blast_db = db_path, task = \"megablast\")\n",
    "    # # 53 seconds\n",
    "    # @time dc_megablast_report = Mycelia.run_blastn(out_dir = SRR_path, fasta = assembled_fasta, blast_db = db_path, task = \"dc-megablast\")\n",
    "    # 98 seconds\n",
    "    # 585.139944 seconds\n",
    "    @time blastn_report = Mycelia.run_blastn(out_dir = SRR_path, fasta = assembled_fasta, blast_db = db_path, task = blastn_task)\n",
    "\n",
    "    # 7747 contigs total\n",
    "    # # 286 hits against 210 contigs\n",
    "    # megablast_results = Mycelia.parse_blast_report(megablast_report)\n",
    "    # @show DataFrames.nrow(megablast_results)\n",
    "    # @show length(unique(megablast_results[!, \"query id\"]))\n",
    "\n",
    "    # # 1370 hits across 625 contigs\n",
    "    # dc_megablast_results = Mycelia.parse_blast_report(dc_megablast_report)\n",
    "    # @show DataFrames.nrow(dc_megablast_results)\n",
    "    # @show length(unique(dc_megablast_results[!, \"query id\"]))\n",
    "\n",
    "    # # 2432 hits across 900 contigs\n",
    "    @info \"reading in blast report\"\n",
    "    blastn_results = Mycelia.parse_blast_report(blastn_report)\n",
    "    # @show DataFrames.nrow(blastn_results)\n",
    "    # @show length(unique(blastn_results[!, \"query id\"]))\n",
    "\n",
    "    # viral_contigs = Set(unique(blastn_results[!, \"query id\"]));\n",
    "    # # take everything that is a sensitive blastn hit against the viruses db, and then blast those against NT to verify there aren't better non-viral hits\n",
    "    # viral_fasta = replace(assembled_fasta, \".fna\" => \".viral.fna\")\n",
    "    # open(viral_fasta, \"w\") do io\n",
    "    #     fastx_io = FASTX.FASTA.Writer(io)\n",
    "    #     for record in FASTX.FASTA.Reader(open(assembled_fasta))\n",
    "    #         if (FASTX.identifier(record) in viral_contigs)\n",
    "    #             write(fastx_io, record)\n",
    "    #         end\n",
    "    #     end\n",
    "    #     close(fastx_io)\n",
    "    # end\n",
    "    # @assert length(collect(Mycelia.open_fastx(viral_fasta))) == length(viral_contigs)\n",
    "\n",
    "    # nt_blastdb_dir = \"$(homedir())/workspace/blastdb\"\n",
    "    # nt_blast_db = \"nt\"\n",
    "    # nt_blastdb_path = joinpath(nt_blastdb_dir, nt_blast_db)\n",
    "    # if isdir(blastdb_dir) && !isempty(readdir(blastdb_dir))\n",
    "    #     @info \"blast db detected, using existing\"\n",
    "    # else\n",
    "    #     Mycelia.download_blast_db(db=nt_blast_db, outdir=nt_blastdb_dir, source=\"ncbi\")\n",
    "    # end\n",
    "\n",
    "    # @time blastn_nt_report = Mycelia.run_blastn(out_dir = SRR_path, fasta = viral_fasta, blast_db = nt_blastdb_path, task = \"blastn\")\n",
    "    # errored out...\n",
    "\n",
    "    # this actually had more viral hits than blastn against representative viruses, but only by a bit more\n",
    "    # nt_megablast_report = joinpath(data_dir, \"SRA/SRR6399459/blastn/final.contigs.fastg.gfa.fna.blastn.nt.megablast.txt\")\n",
    "\n",
    "    # intersect(hits, parse.(Int, blastn_results[!, \"query id\"]))\n",
    "\n",
    "    # # 18741252 hits (too many!!) across 1142 contigs (not enough?!)\n",
    "    # # only 17 contigs came back with viral classifications!!\n",
    "    # nt_megablast_results = Mycelia.parse_blast_report(nt_megablast_report)\n",
    "    # # @show DataFrames.nrow(nt_megablast_results)\n",
    "    # # @show length(unique(nt_megablast_results[!, \"query id\"]))\n",
    "\n",
    "    # create filtered list of contigs not identified by first pass\n",
    "    # run dc-megablast to get second quick pass at genus-level identifications\n",
    "\n",
    "    # read fasta file to determine list of contigs\n",
    "    # fastx_identifiers = [FASTX.identifier(record) for record in FASTX.FASTA.Reader(open(assembled_fasta))]\n",
    "    # megablast_hits = unique(map(x -> first(split(x, '\\t')), Iterators.filter(l -> !occursin(r\"^#\", l), eachline(megablast_report))))\n",
    "    # not_yet_annotated = setdiff(fastx_identifiers, megablast_hits)\n",
    "    # not_yet_annotated_fasta = replace(assembled_fasta, \".fna\" => \".not_yet_annotated.fna\")\n",
    "    # open(not_yet_annotated_fasta, \"w\") do io\n",
    "    #     fastx_io = FASTX.FASTA.Writer(io)\n",
    "    #     for record in FASTX.FASTA.Reader(open(assembled_fasta))\n",
    "    #         if (FASTX.identifier(record) in not_yet_annotated)\n",
    "    #             write(fastx_io, record)\n",
    "    #         end\n",
    "    #     end\n",
    "    #     close(fastx_io)\n",
    "    # end     \n",
    "    # megablast_results = Mycelia.parse_blast_report(megablast_report)\n",
    "\n",
    "    # Mycelia.run_blastn(out_dir = SRR_path, fasta = assembled_fasta, blast_db = db_path, task = \"dc-megablast\")\n",
    "\n",
    "    # create filered list of contigs not identified by first or second pass\n",
    "\n",
    "    # Mycelia.run_blastn(out_dir = SRR_path, fasta = assembled_fasta, blast_db = db_path, task = \"blastn\")\n",
    "\n",
    "    # @time blast_report = Mycelia.run_blast(out_dir = SRR_path, fasta = assembled_fasta, blast_db = db_path, blast_command = \"blastn\")\n",
    "\n",
    "    detected_tax_id_file = \"$(assembled_fasta).detected_tax_ids.txt\"\n",
    "    open(detected_tax_id_file, \"w\") do io\n",
    "        for taxid in unique(filter(!ismissing, blastn_results[!, \"subject tax id\"]))\n",
    "            println(io, taxid)\n",
    "        end\n",
    "    end\n",
    "    taxid_to_lineage_map = Dict(parse(Int, split_line[1]) => split_line[2] for split_line in split.(readlines(`taxonkit lineage --data-dir $(taxdump_dir) $(detected_tax_id_file)`), '\\t'))\n",
    "\n",
    "    qualimap_report_txt = joinpath(SRR_path, \"megahit\", \"qualimap\", \"genome_results.txt\")\n",
    "    qualimap_contig_coverage_table = Mycelia.parse_qualimap_contig_coverage(qualimap_report_txt)\n",
    "\n",
    "    blastn_results[!, \"lineage\"] = map(x -> get(taxid_to_lineage_map, x, \"\"), blastn_results[!, \"subject tax id\"])\n",
    "    blastn_results[!, \"% of subject length\"] = round.(blastn_results[!, \"query length\"] ./ blastn_results[!, \"subject length\"] * 100, digits=3)\n",
    "    contig_info_table = DataFrames.leftjoin(qualimap_contig_coverage_table, blastn_results, on=\"Contig\" => \"query id\")\n",
    "\n",
    "    # re-order columns based on utility\n",
    "    reordered_columns = [\n",
    "        \"Contig\",\n",
    "        \"Length\",\n",
    "        \"Mapped bases\",\n",
    "        \"Mean coverage\",\n",
    "        \"Standard Deviation\",\n",
    "        \"% Mapped bases\",\n",
    "        \"subject id\",\n",
    "        \"subject acc.\",\n",
    "        \"subject title\",\n",
    "        \"subject tax id\",\n",
    "        \"lineage\",\n",
    "        \"% identity\",\n",
    "        \"% of subject length\",\n",
    "        \"evalue\",\n",
    "        \"bit score\",\n",
    "        \"query length\",\n",
    "        \"subject length\",\n",
    "        \"alignment length\",\n",
    "        \"q. start\",\n",
    "        \"q. end\",\n",
    "        \"s. start\",\n",
    "        \"s. end\",\n",
    "        \"identical\",\n",
    "        \"mismatches\"\n",
    "    ]\n",
    "    contig_info_table = contig_info_table[!, reordered_columns]\n",
    "    sort!(contig_info_table, [\"% Mapped bases\", \"bit score\"], rev=true)\n",
    "    # consider gzipping for large files!\n",
    "    uCSV.write(contig_info_tsv, coalesce.(contig_info_table, \"\"), delim='\\t')\n",
    "end\n",
    "\n",
    "# use 3 way triangulation - reads classify to viral\n",
    "# contigs classify to viral\n",
    "# reads map to contigs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
