{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0523df2e-b25a-4efe-8869-87b3c41d123b",
   "metadata": {},
   "source": [
    "The objective of this notebook is to cross-validate the 2 different NCBI blast mappings meant to identify viral contigs with read-level classifications produced by Kraken\n",
    "\n",
    "Contigs will be filtered to only those that are classified as viral by NCBI blast against the \"ref_viruses_rep_genomes\" blast database using 2 different algorithms\n",
    "- blastn\n",
    "    - this is expected to be overly generous\n",
    "    - any contig NOT identified by this algorithm as being potentially viral is dropped automatically, but we expect more false positives than false negatives\n",
    "- dc-megablast\n",
    "    - this is expected to be close to correct\n",
    "    - there may still be some false positives in this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce59cd77-c0f0-4b90-8de8-3c5f071935ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.activate(\".\")\n",
    "\n",
    "# not for the faint of heart!\n",
    "# Pkg.update()\n",
    "\n",
    "pkgs = [\n",
    "\"ArgParse\",\n",
    "\"Base64\",\n",
    "\"BioSequences\",\n",
    "\"DataFrames\",\n",
    "\"Dates\",\n",
    "\"DelimitedFiles\",\n",
    "\"FASTX\",\n",
    "\"GLM\",\n",
    "\"HTTP\",\n",
    "\"JSON\",\n",
    "\"Graphs\",\n",
    "\"MetaGraphs\",\n",
    "\"MD5\",\n",
    "\"Statistics\",\n",
    "\"StatsPlots\",\n",
    "\"uCSV\",\n",
    "\"CodecZlib\",\n",
    "\"YAML\",\n",
    "\"Revise\",\n",
    "\"Kmers\",\n",
    "\"StatsBase\",\n",
    "\"ProgressMeter\",\n",
    "\"XAM\",\n",
    "\"DataStructures\"\n",
    "]\n",
    "Pkg.add(pkgs)\n",
    "for pkg in pkgs\n",
    "    eval(Meta.parse(\"import $pkg\"))\n",
    "end\n",
    "\n",
    "import Mycelia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909f13b4-7e9d-429b-8b83-33c99af653c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = joinpath(dirname(pwd()), \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf0fdc-def3-44bf-b0ba-58ef182cdeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "blastdbs_dir = \"$(homedir())/workspace/blastdbs\"\n",
    "taxdump_dir = mkpath(joinpath(blastdbs_dir, \"taxdump\"))\n",
    "taxdump_tar = joinpath(taxdump_dir, \"taxdump.tar.gz\")\n",
    "if !isfile(taxdump_tar)\n",
    "    run(`wget --quiet https://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz --directory-prefix=$(taxdump_dir)`)\n",
    "end\n",
    "if isempty(filter(x -> occursin(r\"\\.dmp$\", x), readdir(taxdump_dir)))\n",
    "    run(`tar -xvzf $(taxdump_tar) --directory $(taxdump_dir)`)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4a27fb-9f40-4121-9c91-b87716eea130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # blast_db = \"nt\"\n",
    "# # note since starting this, there is now an nv_viruses blast db that was not available when I started this\n",
    "# blast_db = \"nt_viruses\"\n",
    "# # blast_db = \"ref_viruses_rep_genomes\"\n",
    "# blastdb_dir = mkpath(joinpath(blastdbs_dir, blast_db))\n",
    "# # if !isempty(readdir(blastdb_dir))\n",
    "#     # @info \"blast db detected, using existing\"\n",
    "# # else\n",
    "# # Mycelia.download_blast_db(db=blast_db, outdir=blastdb_dir, source=\"ncbi\")\n",
    "# # end\n",
    "\n",
    "# db_path = joinpath(blastdb_dir, blast_db)\n",
    "# blastn_task = \"megablast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1927f3-4ed2-45e5-ada9-be4bfdb0a419",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRR_paths = filter(x -> !occursin(\".ipynb_checkpoints\", x), readdir(joinpath(data_dir, \"SRA\"), join=true))\n",
    "SRR_paths = filter(x -> isfile(joinpath(x, \"megahit\", \"final.contigs.fastg.gfa.fna\")), SRR_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2569f9-e908-4d52-95c6-fd6dd15d8c07",
   "metadata": {},
   "source": [
    "The following was a manual, indepth analysis of the first sample that we used to inform our validation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2e4b7f-56b3-4ea3-9f93-a2541f067dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract contigs that came back as viral in the targetted screen\n",
    "# # blast back against NCBI nt and confirm they are still viral\n",
    "# SRR_path = first(SRR_paths)\n",
    "# SRR = basename(SRR_path)\n",
    "\n",
    "# # ref_viruses_ref_genomes_blast_report = joinpath(SRR_path, \"blastn\", \"final.contigs.fastg.gfa.fna.blastn.ref_viruses_rep_genomes.blastn.txt\")\n",
    "# # ref_viruses_ref_genomes_blast_results = Mycelia.parse_blast_report(ref_viruses_ref_genomes_blast_report)\n",
    "# # possible_viral_contigs = Set(unique(ref_viruses_ref_genomes_blast_results[!, \"query id\"]))\n",
    "# # assembled_fasta = joinpath(SRR_path, \"megahit\", \"final.contigs.fastg.gfa.fna\")\n",
    "# # viral_fasta = replace(assembled_fasta, \".fna\" => \".potential_viral_contigs.fna\")\n",
    "# # potential_viral_records = filter(x -> FASTX.identifier(x) in possible_viral_contigs, collect(Mycelia.open_fastx(assembled_fasta)))\n",
    "# # open(viral_fasta, \"w\") do io\n",
    "# #     fastx_io = FASTX.FASTA.Writer(io)\n",
    "# #     for record in potential_viral_records\n",
    "# #         write(fastx_io, record)\n",
    "# #     end\n",
    "# #     close(fastx_io)\n",
    "# # end\n",
    "\n",
    "# # potential_viral_records\n",
    "\n",
    "# # 20 days of runtime using full NT database just on viral contigs subset :(\n",
    "# # # seconds per sample * samples\n",
    "# # total_runtime =  (3000 * 600)\n",
    "\n",
    "# # # 60 seconds per minute, 60 minutes per hour, 24 hours per day\n",
    "# # days_of_runtime = total_runtime / 60 / 60 / 24\n",
    "\n",
    "# # nt_megablast_file = \"final.contigs.fastg.gfa.fna.blastn.nt.megablast.txt\"\n",
    "# ref_viruses_rep_genomes_blastn_file = \"final.contigs.fastg.gfa.fna.blastn.ref_viruses_rep_genomes.blastn.txt\"\n",
    "# ref_viruses_rep_genomes_dcmegablast_file = \"final.contigs.fastg.gfa.fna.blastn.ref_viruses_rep_genomes.dc-megablast.txt\"\n",
    "# ref_viruses_rep_genomes_megablast_file = \"final.contigs.fastg.gfa.fna.blastn.ref_viruses_rep_genomes.megablast.txt\"\n",
    "# nt_viral_validation_megablast_file = \"final.contigs.fastg.gfa.potential_viral_contigs.fna.blastn.nt_viruses.megablast.txt\"\n",
    "# nt_validation_megablast_file = \"final.contigs.fastg.gfa.potential_viral_contigs.fna.blastn.nt.megablast.txt\"\n",
    "\n",
    "# # nt_megablast_results = Mycelia.parse_blast_report(joinpath(SRR_path, \"blastn\", nt_megablast_file))\n",
    "# ref_viruses_rep_genomes_blastn_results = Mycelia.parse_blast_report(joinpath(SRR_path, \"blastn\", ref_viruses_rep_genomes_blastn_file))\n",
    "# ref_viruses_rep_genomes_dcmegablast_results = Mycelia.parse_blast_report(joinpath(SRR_path, \"blastn\", ref_viruses_rep_genomes_dcmegablast_file))\n",
    "# ref_viruses_rep_genomes_megablast_results = Mycelia.parse_blast_report(joinpath(SRR_path, \"blastn\", ref_viruses_rep_genomes_megablast_file))\n",
    "# nt_viral_validation_megablast_results = Mycelia.parse_blast_report(joinpath(SRR_path, \"blastn\", nt_viral_validation_megablast_file))\n",
    "# nt_validation_megablast_results = Mycelia.parse_blast_report(joinpath(SRR_path, \"blastn\", nt_validation_megablast_file))\n",
    "\n",
    "# # most generous\n",
    "# # ref_viruses_rep_genomes_blastn_results\n",
    "# # ref_viruses_rep_genomes_dcmegablast_results\n",
    "# # ref_viruses_rep_genomes_megablast_results\n",
    "# # nt_viral_validation_megablast_results\n",
    "# # nt_validation_megablast_results\n",
    "# # most strict\n",
    "\n",
    "# taxon_id_to_kingdom_map = Dict{Int, String}()\n",
    "# kingdom_to_taxon_id_map = Dict(\n",
    "#     \"Viruses\" => 10239,\n",
    "#     \"Archaea\" => 2157,\n",
    "#     \"Bacteria\" => 2,\n",
    "#     \"Eukaryota\" => 2759,\n",
    "#     \"Other\" => 28384,\n",
    "#     \"Unclassified\" => 12908\n",
    "# )\n",
    "# ProgressMeter.@showprogress for (kingdom, taxon_id) in kingdom_to_taxon_id_map\n",
    "#     for child_taxon_id in parse.(Int, filter(!isempty, readlines(`taxonkit list --data-dir $(taxdump_dir) --ids $(taxon_id) --indent=\"\"`)))\n",
    "#         taxon_id_to_kingdom_map[child_taxon_id] = kingdom\n",
    "#     end\n",
    "# end\n",
    "\n",
    "# ref_viruses_rep_genomes_blastn_contigs = unique(ref_viruses_rep_genomes_blastn_results[!, \"query id\"])\n",
    "\n",
    "# ref_viruses_rep_genomes_dcmegablast_contigs = unique(ref_viruses_rep_genomes_dcmegablast_results[!, \"query id\"])\n",
    "\n",
    "# # # 8 non-overlapping hits!\n",
    "# # union(ref_viruses_rep_genomes_dcmegablast_contigs, ref_viruses_rep_genomes_blastn_contigs)\n",
    "# # # these all have pretty low e-values, I'm not going to worry about them\n",
    "# # dcmegablast_only = setdiff(ref_viruses_rep_genomes_dcmegablast_contigs, ref_viruses_rep_genomes_blastn_contigs)\n",
    "# # ref_viruses_rep_genomes_dcmegablast_results[map(x -> x in dcmegablast_only, ref_viruses_rep_genomes_dcmegablast_results[!, \"query id\"]), :]\n",
    "\n",
    "# ref_viruses_rep_genomes_megablast_contigs = unique(ref_viruses_rep_genomes_megablast_results[!, \"query id\"])\n",
    "# # nt_viral_validation_megablast_results\n",
    "# # nt_validation_megablast_results\n",
    "# # all megablast hits are subset of blastn hits\n",
    "# # union(ref_viruses_rep_genomes_megablast_contigs, ref_viruses_rep_genomes_blastn_contigs)\n",
    "\n",
    "# nt_viral_validation_megablast_contigs = unique(nt_viral_validation_megablast_results[!, \"query id\"])\n",
    "# # nt_validation_megablast_results\n",
    "\n",
    "# # union(nt_viral_validation_megablast_contigs, ref_viruses_rep_genomes_blastn_contigs)\n",
    "# # map(x -> x in viral_taxon_ids, nt_viral_validation_megablast_results[!, \"subject tax id\"])\n",
    "\n",
    "# taxon_id_to_kingdom_map\n",
    "\n",
    "# # only 12 contigs are still considered viral contigs after mapping to nt complete, but the hits aren't very convicing (bacterial artificial chromosomes?)\n",
    "# nt_validation_megablast_contigs = unique(nt_validation_megablast_results[map(x -> get(taxon_id_to_kingdom_map, x, \"\") == \"Viruses\", nt_validation_megablast_results[!, \"subject tax id\"]), \"query id\"])\n",
    "\n",
    "# full_contig_set = union(nt_validation_megablast_contigs, nt_viral_validation_megablast_contigs, ref_viruses_rep_genomes_blastn_contigs, ref_viruses_rep_genomes_dcmegablast_contigs, ref_viruses_rep_genomes_megablast_contigs)\n",
    "\n",
    "# # get all of the reads mapping to each\n",
    "\n",
    "# bam_file = joinpath(SRR_path, \"megahit\", \"final.contigs.fastg.gfa.fna.bwa.bam\")\n",
    "# # bamfile = first(filter(x -> occursin(r\"\\.bam$\", x), readdir(joinpath(SRR_dir, \"megahit\"), join=true)))\n",
    "\n",
    "# # implement as the following nested dictionary\n",
    "# # contigs => reads => taxon_id\n",
    "\n",
    "# function generate_contig_to_reads_map(bamfile, contigs_of_interest)\n",
    "#     contigs_to_reads_map = Dict(contig => Set{String}() for contig in contigs_of_interest)\n",
    "#     # reads_of_interest = Set{String}()\n",
    "#     reader = open(XAM.BAM.Reader, bamfile)\n",
    "#     for record in reader\n",
    "#         if XAM.BAM.ismapped(record) && (XAM.BAM.refname(record) in contigs_of_interest)\n",
    "#             push!(contigs_to_reads_map[XAM.BAM.refname(record)], XAM.BAM.tempname(record))\n",
    "#         end\n",
    "#     end\n",
    "#     close(reader)\n",
    "#     return contigs_to_reads_map\n",
    "# end\n",
    "\n",
    "# # 1200 seconds\n",
    "# @time contigs_to_reads_map = generate_contig_to_reads_map(bam_file, full_contig_set)\n",
    "\n",
    "# function read_kraken_output(kraken_output)\n",
    "#     # read_kraken_report\n",
    "#     header = [\n",
    "#         \"classification status\",\n",
    "#         \"sequence ID\",\n",
    "#         \"taxon ID\",\n",
    "#         \"sequence length\",\n",
    "#         \"LCA mappings\"\n",
    "#     ]\n",
    "#     data, _ = uCSV.read(IOBuffer(join(filtered_lines, '\\n')), delim='\\t')\n",
    "#     return DataFrames.DataFrame(data, header)\n",
    "# end\n",
    "\n",
    "# # readdir(joinpath(SRR_path, \"kraken\"))\n",
    "\n",
    "# reads_of_interest = reduce(union, values(contigs_to_reads_map))\n",
    "\n",
    "# kraken_output = last(filter(x -> occursin(r\"\\.kraken-output\\.tsv\", x), readdir(joinpath(SRR_path, \"kraken\"), join=true)))\n",
    "# if occursin(r\"\\.gz\", kraken_output)\n",
    "#     kraken_buffer = open(`gzip -dc $(kraken_output)`)\n",
    "# else\n",
    "#     kraken_buffer = open(kraken_output)\n",
    "# end\n",
    "\n",
    "# filtered_lines = String[]\n",
    "# for line in eachline(kraken_buffer)\n",
    "#     split_line = split(line, '\\t')\n",
    "#     if split_line[2] in reads_of_interest\n",
    "#         push!(filtered_lines, line)\n",
    "#     end\n",
    "# end\n",
    "# close(kraken_buffer)\n",
    "\n",
    "# read_classifications = read_kraken_output(IOBuffer(join(filtered_lines, '\\n')))\n",
    "# read_classifications[!, \"parsed taxon ID\"] = map(x -> match(r\"\\(taxid (\\d+)\\)\", x).captures[1], read_classifications[!, \"taxon ID\"])\n",
    "# read_classifications\n",
    "\n",
    "# read_classifications_map = Dict(row[\"sequence ID\"] => parse(Int, row[\"parsed taxon ID\"]) for row in DataFrames.eachrow(read_classifications))\n",
    "\n",
    "# contigs_to_taxon_counts_map = Dict()\n",
    "# for (contig, reads) in contigs_to_reads_map\n",
    "#     contigs_to_taxon_counts_map[contig] = StatsBase.countmap(get(taxon_id_to_kingdom_map, read_classifications_map[read], \"Unclassified\") for read in reads)\n",
    "# end\n",
    "# contigs_to_taxon_counts_map\n",
    "\n",
    "# contigs_to_taxon_proportions_map = Dict()\n",
    "# for (contig, taxon_counts) in contigs_to_taxon_counts_map\n",
    "#     total_count = sum(values(taxon_counts))\n",
    "#     contigs_to_taxon_proportions_map[contig] = Dict(kingdom => count / total_count for (kingdom, count) in taxon_counts)\n",
    "# end\n",
    "# contigs_to_taxon_proportions_map\n",
    "\n",
    "# # ref_viruses_rep_genomes_blastn_results\n",
    "# # ref_viruses_rep_genomes_dcmegablast_results\n",
    "# # ref_viruses_rep_genomes_megablast_results\n",
    "# # nt_viral_validation_megablast_results\n",
    "# # nt_validation_megablast_results\n",
    "\n",
    "# contig_classification_results = \n",
    "# DataFrames.DataFrame(\n",
    "#     union(\n",
    "#         DataStructures.OrderedDict(\"Contig\" => String[]),\n",
    "#         DataStructures.OrderedDict(k => Float64[] for k in keys(kingdom_to_taxon_id_map)),\n",
    "#         # these are ordered by most hits to fewest hits\n",
    "#         DataStructures.OrderedDict(db_algorithm => Bool[] for db_algorithm in [\"ref_viruses_rep_genomes_blastn\", \"ref_viruses_rep_genomes_dcmegablast\", \"nt_viral_validation_megablast\", \"ref_viruses_rep_genomes_megablast\", \"nt_validation_megablast\"])\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# for (contig, taxon_proportions) in contigs_to_taxon_proportions_map\n",
    "#     row = Dict{Any, Any}(\"Contig\" => contig)\n",
    "#     for k in keys(kingdom_to_taxon_id_map)\n",
    "#         row[k] = get(taxon_proportions, k, 0.0)\n",
    "#     end\n",
    "#     row[\"ref_viruses_rep_genomes_blastn\"] = contig in ref_viruses_rep_genomes_blastn_contigs\n",
    "#     row[\"nt_viral_validation_megablast\"] = contig in nt_viral_validation_megablast_contigs\n",
    "#     row[\"ref_viruses_rep_genomes_dcmegablast\"] = contig in ref_viruses_rep_genomes_dcmegablast_contigs\n",
    "#     row[\"ref_viruses_rep_genomes_megablast\"] = contig in ref_viruses_rep_genomes_megablast_contigs\n",
    "#     row[\"nt_validation_megablast\"] = contig in nt_validation_megablast_contigs\n",
    "#     push!(contig_classification_results, row)\n",
    "# end\n",
    "# contig_classification_results[!, \"top_kingdom\"] .= \"\"\n",
    "# for (i, row) in enumerate(DataFrames.eachrow(contig_classification_results))\n",
    "#     max_hit = \"\"\n",
    "#     max_value = 0.0\n",
    "#     for k in keys(kingdom_to_taxon_id_map)\n",
    "#         if row[k] > max_value\n",
    "#             max_value = row[k]\n",
    "#             max_hit = k\n",
    "#         end\n",
    "#     end\n",
    "#     contig_classification_results[i, \"top_kingdom\"] = max_hit\n",
    "# end\n",
    "\n",
    "# m = Int.(Matrix(\n",
    "#     contig_classification_results[!, [\n",
    "#         \"ref_viruses_rep_genomes_blastn\",\n",
    "#         \"ref_viruses_rep_genomes_dcmegablast\",\n",
    "#         \"nt_viral_validation_megablast\",\n",
    "#         \"ref_viruses_rep_genomes_megablast\",\n",
    "#         \"nt_validation_megablast\"\n",
    "#     ]\n",
    "# ]))\n",
    "\n",
    "# contig_classification_results[!, \"blast_hits\"] = map(r -> sum(r), eachrow(m))\n",
    "\n",
    "# contig_classification_results\n",
    "\n",
    "# # eukaryotic top hit - kraken is asserting these are human contamination\n",
    "# sum(contig_classification_results[!, \"Eukaryota\"])\n",
    "\n",
    "# # novel sequences are second hit, this is exciting!\n",
    "# sum(contig_classification_results[!, \"Unclassified\"])\n",
    "\n",
    "# sum(contig_classification_results[!, \"Bacteria\"])\n",
    "\n",
    "# sum(contig_classification_results[!, \"Other\"])\n",
    "\n",
    "# sum(contig_classification_results[!, \"Viruses\"])\n",
    "\n",
    "# sum(contig_classification_results[!, \"Archaea\"])\n",
    "\n",
    "# contig_classification_results_summary = contig_classification_results[!, [\"Contig\", \"top_kingdom\", \"blast_hits\"]]\n",
    "\n",
    "# contig_classification_results_summary\n",
    "\n",
    "# StatsBase.countmap(contig_classification_results_summary[!, \"blast_hits\"])\n",
    "\n",
    "# contig_classification_results_summary[contig_classification_results_summary[!, \"blast_hits\"] .== 5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6655bbe-5035-4621-89ba-4335dba973fa",
   "metadata": {},
   "source": [
    "Based on these results, we will go for a 3x confirmation via targetted blast where the sequence must have a high confidence match for a viral hit using:\n",
    "- ref_viruses_rep_genomes_blastn\n",
    "- ref_viruses_rep_genomes_dcmegablast\n",
    "- nt_viral_validation_megablast\n",
    "\n",
    "Full nt database attempts to validate viral contigs came back primarily with bacterial artificial chromosomes associated with human cell lines that did not seem like valuable hits to us. We also didn't expect to find *novel* viruses by megablasting against the ref_viruses_rep_genomes, given that it is a limited, representative set of all known viruses and the official description of the algorithm is \"Traditional megablast used to find very similar (e.g., intraspecies or closely related species) sequences\"\n",
    "https://www.ncbi.nlm.nih.gov/books/NBK569839/\n",
    "\n",
    "Because of this, we felt that getting a multiple redudant hits using the most flexibile algorithm (blastn) against the highest quality blast db (ref_viruses_rep_genomes), a less flexible, but still cross-species algorithm (dc-megablast: Discontiguous megablast used to find more distant (e.g., interspecies) sequences), and finally a 3rd validation hit against a potentially lower quality due to less manual curation viral database (nt_viral) using the strictest alogrithm, megablast.\n",
    "\n",
    "Because of the low concordance of Kraken classifications to blast classifications, we consider the calls with much less weight than the classified contigs, but include them because the data was generated and it may prove informative to others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6686068b-a98d-4a1e-9719-c1891057396b",
   "metadata": {},
   "source": [
    "As an update to the above, we found that we were unable to perform the `nt_viral_validation_megablast` in a reasonable amount of time, so we will be dropping that requirement.\n",
    "\n",
    "Instead, we will perform a final re-assembly and then perform the final validation blast and protein-level classification on that final assembly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
