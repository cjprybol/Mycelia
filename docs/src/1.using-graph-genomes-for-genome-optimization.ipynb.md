
# Using graph genomes for genome optimization


## Introduction

Now that we have the ability to sequence as many genomes as the research community can afford to pay
for, and that there are so many genomes publicly available online, we are now
in a situation where we can learn what regions of genomes contribute to what
kind of phenotypic properties by associating measured attributes with the
co-occurrence of genetic features even for non-model and unculturable
organisms. When building these models initially, there are no causal attributes
that can necessarily be attributed because we are simply looking for
correlative relationships of phenotypic traits appearing in the presence of or
in the absence of specific genetic features. Given the ability to associate
genetic features with phenotypic features that may be desirable or undesirable,
we are given the opportunity to use graph pan genomes as vehicles for genetic
optimization driven entirely by data. These non-causal relationships can be
expanded upon over time with additional information that is derived from causal
analysis conducted statistically and/or via direct experimentation.

Given a weighted
graph structure that enables the attribution of specific expected properties to
paths through the graph (genomes, transcriptomes, proteomes), there may be
situations in which it may be desirable to modify and manipulate genomes to
have them manifest these desirable characteristics in an actual organism. For
example, it may be desired to encode features that are most consistent with
outcomes such as health and longevity (humans), avirulence/lack of
pathogenicity (pathogens/microbes/viruses), as well as having the smallest
essential (minimal) genome (biomanufacturing).

Given a
pangenomegraph with weights where the weights are proportional to an
optimizeable measure such as correlation, likelihood, frequency, etc., the
question becomes how do we find the optimal path through this genome graph such
that the average score is maximized so that we can test whether the predictive
value of the graph can manifest the desired outcomes if the genome was encoded
in a living organism.

At the time of
writing, the optimization algorithms that I am aware of only operate on the
weights of the edges. For example, there are many optimization algorithms that
find the shortest path between two nodes. Examples include A-star, djikstra,
bellman-Ford, traveling salesman problem (note that traveling salesman problem
assumes a cyclic optimization which is very helpful specifically when dealing
with circular genomes such as those of phage bacteria or other prokaryotic
organisms). Given that we don’t have a plethora of optimization algorithms that
account for edge weights and node weights (aside from hidden Markov models
which become impractical at very large graph sizes common in pangenomics), we
will look at the node optimization and path optimization components of our goal
individually. 

We will first ask
the question "which nodes do we want to include?", which can be done
by setting a threshold and only retaining nodes that meet that threshold,
clustering (supervised and/or unsupervised), and likely many more methods.
Given the set of nodes of interest, we can induce a subgraph containing just
those nodes and then find the optimal or a sufficiently optimal path through
the subset of remaining nodes that maximizes the overall optimization that we
are trying to achieve.

For the problem of
trying to find the minimally sized genome, our node optimization is relative
frequency of genes contained in all known entities of a given species or other
phylogenetic level of classification (threshold = must be contained in 100% of observed
entities). In this same problem, the path optimization is the shortest viable
path of observed possible paths that includes all of these nodes. Further
possible optimization can be done if we are willing to consider connections
that have not yet been observed (e.g. gene fusions and assumptions that
non-coding elements between genes are themselves non-essential).

To create the
structure necessary for this optimization, we will first build a pan genome
graph that includes the DNA kmers, gene annotations + amino acid kmers, and the
pathways that they encode. We can then look at the relative feature space in
all three of these levels. The highest and most abstract level is that of the
metabolic pathway (Note, in an ideal world all metabolic pathways would be
fully resolved and validated and in such a world this would be very
non-abstract, however in practice we are unable to unambiguously resolve and
verify all metabolic pathways in all organisms and therefore I will consider it
in the abstract sense for the purposes of this explanation). 

[figure here showing
pathways and relative frequency of conservation among a group of entities]

At a functional
protein level, the goal is not necessarily to consider all possible proteins as
unique variants but rather to cluster the proteins into functionally equivalent
clades/families, ask which clades/families maximize our intended value (for minimal
genome size, frequency in all entitities considered = 100%), and then after the
clades have been selected, choose the optimal known variant of that clade that
can then be selected in the AA space and further constrained into the DNA space
based on codon frequency profiles. 

[insert figure here
showing protein families with relative frequencies and amino acid variability
that then translates down into DNA space and alternate codon options that can
be optimized again]

At the lowest level,
we have the DNA itself where our goal is simply to try and maximize the
optimization value of choice through all possible DNA pads in the graph until
we have either created a completely circular circular rise full genome or we
have reached the end of the chromosome all telomeres.

[insert figure here
showing the traversal of a graph that either loops into a complete circle or
terminates at terminal telomeres]

In all cases the
final minimal form must go through a DNA genome (with the exception of
RNA-based viruses, which would follow a conceptually equivalent format but
would build on a foundation of an RNA pangenome/pantranscriptome). The key
advantage of constraining in the pathway and protein layers of abstraction is
the dimensionality reduction provided. A genome may contain billions of nucleic
acid bases, but is likely to only contain up to thousands of pathways and
thousands of genes. These constraints will enable more efficient optimization
of incredibly large and complex pangenomes such as those of complex microbiomes
(e.g. soil) and large eukaryotic genomes (e.g. humans and many crops).

Given a set of
constrained points, which may or may not be equivalent to the set of all
possible points depending on what kind of optimization steps were carried out
before, we then try and find the optimal path between points in a stepwise
(random or greedy) order and ask first "is there a path between this next
node and any of the current nodes in the graph?" and if so, return that
path and add this node to this graph. If not, add this node to the set of nodes
in the graph and await connections to the remaining nodes yet to be evaluated.

[insert a figure
showing this workflow where we have a full graph, a reduced set of nodes that
meet the thresholding criteria, and then iterative making the connections
between them]

(personal note,
maybe could also use the Bellman-Ford on a given induced subgraph and just
iteratively pick out the shortest connections?)

For the genome
optimization problem, different sections of the graph will need to be
sequestered away and isolated into chromosomes as we reach terminating steps. I
think there are several possible terminating steps worth considering. One
possible consideration is that we create a fully circularized chromosome. In
the event of this happens, this should be an end condition for that
chromosomes. When considering linear chromosomes, if we hit a terminal node in
the graph in two locations, equivalent to reaching the telomeres biologically,
we should sequester this node collection/chromosome off and remove considering
additions to this chromosome for all future inclusions to our optimized graph.

When trying to
decide if you’re multichromosome all genome is optimally complete I think there
should be a sigmoidal binary supervised cluster that or something simpler such
as a absolute threshold in whereby either we were able to segregate into two
clear populations of optimal and non-optimal or we are able to fit a
diminishing returns Vmax V not midpoint where we feel good that we have
obtained half of or at least half of the core density of the system and we
should let people modify this parameter to their suiting however by default we
have it set to the larger of the 2K means clusters.


## Experiment

Create a compressed reference graph

theoretical:
3 pathways, two of which are important
3 proteins per pathway
1 sequence per 3 proteins
3 random sequences of length 100 - 1000
assign random importance weights to pathways

applied, known answer:
minimal mycobacterium
minimal synthetic yeast chromosome

applied, unknown answer:
phage pathways
phage vogs
the largest genus of bacteriophage (EC tequatrovirus)

initial pass, work at the pathway level
then start again, working at only the protein level
then start again, working only at the dna level


## Results

Visualize the graph before and after simplification


## Conclusions

Assuming a direct, linear relationship between # of genes and cost on potentially wasteful metabolic expenditure, then every gene removal can increase the relative production of remaining metabolites.

While regulatory feedback loops make a direct relationship unlikely, if we operate under this train of thought, then creating minimal genomes should make organisms more efficient and plyable/flexible for bioengineering and minimize the likelihood of the production of any undesirable phenotypes and metabolic outputs if used as biofactories of pharmaceuticals.
