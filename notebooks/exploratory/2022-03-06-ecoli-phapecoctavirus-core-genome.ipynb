{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Proteome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Union{Nothing, SubString{String}}}:\n",
       " \"2022-03-06\"\n",
       " \"ecoli-phapecoctavirus-core-genome\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATE_TASK = \"2022-03-06-ecoli-phapecoctavirus-core-genome\"\n",
    "DIR = mkpath(\"$(homedir())/workspace/$DATE_TASK\")\n",
    "cd(DIR)\n",
    "DATE, TASK = match(r\"^(\\d{4}-\\d{2}-\\d{2})-(.*)$\", DATE_TASK).captures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m git-repo `https://github.com/cjprybol/Mycelia.git#master`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/git/Mycelia/docs/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/git/Mycelia/docs/Manifest.toml`\n",
      "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mCodecBzip2\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mStringEncodings\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMKL_jll\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mGumbo_jll\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mLERC_jll\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mEzXML\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mGumbo\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mXMLDict\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mYAML\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mLibtiff_jll\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mXLSX\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mBioServices\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mGR_jll\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39mWeave\n",
      "\u001b[33m  ✓ \u001b[39m\u001b[90mGR\u001b[39m\n",
      "\u001b[33m  ✓ \u001b[39m\u001b[90mFFTW\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39mBioFetch\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mStatsModels\u001b[39m\n",
      "\u001b[33m  ✓ \u001b[39m\u001b[90mKernelDensity\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mGLM\u001b[39m\n",
      "\u001b[33m  ✓ \u001b[39mPlots\n",
      "\u001b[33m  ✓ \u001b[39mStatsPlots\n",
      "\u001b[33m  ✓ \u001b[39mMycelia\n",
      "  23 dependencies successfully precompiled in 127 seconds (230 already precompiled, 9 skipped during auto due to previous errors)\n",
      "  \u001b[33m6\u001b[39m dependencies precompiled but different versions are currently loaded. Restart julia to access the new versions\n"
     ]
    }
   ],
   "source": [
    "import Pkg\n",
    "Pkg.update()\n",
    "pkgs = [\n",
    "\"JSON\",\n",
    "\"HTTP\",\n",
    "\"Dates\",\n",
    "\"uCSV\",\n",
    "\"DelimitedFiles\",\n",
    "\"DataFrames\",\n",
    "\"ProgressMeter\",\n",
    "\"BioSequences\",\n",
    "\"FASTX\",\n",
    "\"Distances\",\n",
    "\"StatsPlots\",\n",
    "\"StatsBase\",\n",
    "\"Statistics\",\n",
    "\"MultivariateStats\",\n",
    "\"Random\",\n",
    "\"Primes\",\n",
    "\"SparseArrays\",\n",
    "\"SHA\",\n",
    "\"GenomicAnnotations\",\n",
    "\"Combinatorics\",\n",
    "\"OrderedCollections\",\n",
    "\"Downloads\",\n",
    "\"Clustering\",\n",
    "\"Revise\",\n",
    "\"Mmap\",\n",
    "\"LsqFit\",\n",
    "\"BioSymbols\"\n",
    "]\n",
    "\n",
    "for pkg in pkgs\n",
    "    try\n",
    "        eval(Meta.parse(\"import $pkg\"))\n",
    "    catch\n",
    "        Pkg.add(pkg)\n",
    "        eval(Meta.parse(\"import $pkg\"))\n",
    "    end\n",
    "end\n",
    "\n",
    "# works but can't update locally, need to push and restart kernel to activate changes\n",
    "# \"https://github.com/cjprybol/Mycelia.git#master\",\n",
    "# didn't work\n",
    "# \"$(homedir())/git/Mycelia#master\",\n",
    "pkg_path = \"$(homedir())/git/Mycelia\"\n",
    "try\n",
    "    eval(Meta.parse(\"import $(basename(pkg_path))\"))\n",
    "catch\n",
    "    # Pkg.add(url=pkg)\n",
    "    Pkg.develop(path=pkg_path)\n",
    "    # pkg = replace(basename(pkg), \".git#master\" => \"\")\n",
    "    # pkg = replace(basename(pkg), \"#master\" => \"\")\n",
    "    eval(Meta.parse(\"import $(basename(pkg_path))\"))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wcss (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function wcss(clustering_result)\n",
    "    n_clusters = length(clustering_result.counts)\n",
    "    total_squared_cost = 0.0\n",
    "    for cluster_id in 1:n_clusters\n",
    "        cluster_indices = clustering_result.assignments .== cluster_id\n",
    "        total_squared_cost += sum(clustering_result.costs[cluster_indices] .^ 2)\n",
    "    end\n",
    "    return total_squared_cost\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate_all_possible_kmers (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function generate_all_possible_kmers(k, alphabet)\n",
    "    kmer_iterator = Iterators.product([alphabet for i in 1:k]...)\n",
    "    kmer_vectors = collect.(vec(collect(kmer_iterator)))\n",
    "    if eltype(alphabet) == BioSymbols.AminoAcid\n",
    "        kmers = BioSequences.LongAminoAcidSeq.(kmer_vectors)\n",
    "        if k > 1\n",
    "            filter!(kmer -> kmer[1] != BioSequences.AA_Term, kmers)\n",
    "        end\n",
    "    elseif eltype(alphabet) == BioSymbols.DNA\n",
    "        kmers = BioSequences.LongDNASeq.(kmer_vectors)\n",
    "    else\n",
    "        error()\n",
    "    end\n",
    "    return sort!(kmers)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fit_optimal_number_of_clusters (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function fit_optimal_number_of_clusters(distance_matrix)\n",
    "    ks_to_try = vcat([2^i for i in 0:Int(floor(log2(size(distance_matrix, 1))))], size(distance_matrix, 1))\n",
    "    @show ks_to_try\n",
    "    \n",
    "    # can calculate this for k >= 1\n",
    "    # within_cluster_sum_of_squares = Union{Float64, Missing}[]\n",
    "    within_cluster_sum_of_squares = Float64[]\n",
    "    # these are only valid for k >= 2 so set initial value to missing\n",
    "    # between_cluster_sum_of_squares = [missing, zeros(length(ks_to_try)-1)...]\n",
    "    # silhouette_scores = Union{Float64, Missing}[]\n",
    "    silhouette_scores = Float64[]\n",
    "        \n",
    "    current_k_index = 1\n",
    "    @info \"assessing k = $(ks_to_try[current_k_index])\"\n",
    "    this_clustering = Clustering.kmeans(distance_matrix, ks_to_try[current_k_index])\n",
    "    push!(within_cluster_sum_of_squares, wcss(this_clustering))\n",
    "    push!(silhouette_scores, 0)\n",
    "\n",
    "    if length(ks_to_try) == 1\n",
    "        optimal_number_of_clusters = ks_to_try[current_k_index]\n",
    "    else\n",
    "        current_k_index += 1\n",
    "        @info \"assessing k = $(ks_to_try[current_k_index])\"\n",
    "        this_clustering = Clustering.kmeans(distance_matrix, ks_to_try[current_k_index])\n",
    "        push!(within_cluster_sum_of_squares, wcss(this_clustering))\n",
    "        push!(silhouette_scores, Statistics.mean(Clustering.silhouettes(this_clustering, distance_matrix)))\n",
    "        \n",
    "        if (within_cluster_sum_of_squares[2] >= within_cluster_sum_of_squares[1])\n",
    "            optimal_number_of_clusters = ks_to_try[1]\n",
    "        else\n",
    "            optimal_number_of_clusters = ks_to_try[2]\n",
    "            if length(ks_to_try) > 2\n",
    "                current_k_index += 1\n",
    "                @info \"assessing k = $(ks_to_try[current_k_index])\"\n",
    "                this_clustering = Clustering.kmeans(distance_matrix, ks_to_try[current_k_index])\n",
    "                push!(within_cluster_sum_of_squares, wcss(this_clustering))\n",
    "                push!(silhouette_scores, Statistics.mean(Clustering.silhouettes(this_clustering, distance_matrix)))\n",
    "                \n",
    "                while (silhouette_scores[end] > silhouette_scores[end-1]) &&\n",
    "                        (within_cluster_sum_of_squares[end] < within_cluster_sum_of_squares[end-1]) &&\n",
    "                        (current_k_index < length(ks_to_try))\n",
    "                    current_k_index += 1\n",
    "                    @info \"assessing k = $(ks_to_try[current_k_index])\"\n",
    "                    this_clustering = Clustering.kmeans(distance_matrix, ks_to_try[current_k_index])\n",
    "                    push!(within_cluster_sum_of_squares, wcss(this_clustering))\n",
    "                    push!(silhouette_scores, Statistics.mean(Clustering.silhouettes(this_clustering, distance_matrix)))\n",
    "                end\n",
    "                # here is where we should start grid searching within the best range\n",
    "                optimal_silhouette, optimal_index = findmax(silhouette_scores)\n",
    "                optimal_number_of_clusters = ks_to_try[optimal_index]\n",
    "                @info \"refining...\"\n",
    "                @info \"current optimal number of clusters = $(ks_to_try[optimal_index])\"\n",
    "                @info \"current best silhouette score = $(optimal_silhouette)\"\n",
    "                                \n",
    "                if optimal_index != length(ks_to_try)\n",
    "                    window_of_focus = ks_to_try[optimal_index-1:optimal_index+1]\n",
    "                    \n",
    "                    k_to_try = Int(round(Statistics.mean(window_of_focus[1:2])))\n",
    "                    insertion_index = first(searchsorted(ks_to_try, k_to_try))\n",
    "                    if ks_to_try[insertion_index] != k_to_try\n",
    "                        insert!(ks_to_try, insertion_index, k_to_try)\n",
    "                        @info \"assessing k = $(k_to_try)\"\n",
    "                        this_clustering = Clustering.kmeans(distance_matrix, k_to_try)\n",
    "                        insert!(within_cluster_sum_of_squares, insertion_index, wcss(this_clustering))\n",
    "                        insert!(silhouette_scores, insertion_index, Statistics.mean(Clustering.silhouettes(this_clustering, distance_matrix)))\n",
    "                    end\n",
    "\n",
    "                    k_to_try = Int(round(Statistics.mean(window_of_focus[2:3])))\n",
    "                    insertion_index = first(searchsorted(ks_to_try, k_to_try))\n",
    "                    if ks_to_try[insertion_index] != k_to_try\n",
    "                        @info \"assessing k = $(k_to_try)\"\n",
    "                        this_clustering = Clustering.kmeans(distance_matrix, k_to_try)\n",
    "                        insert!(ks_to_try, insertion_index, k_to_try)\n",
    "                        insert!(within_cluster_sum_of_squares, insertion_index, wcss(this_clustering))\n",
    "                        insert!(silhouette_scores, insertion_index, Statistics.mean(Clustering.silhouettes(this_clustering, distance_matrix)))\n",
    "                    end\n",
    "                    \n",
    "                    new_optimal_silhouette, new_optimal_index = findmax(silhouette_scores)\n",
    "                    new_optimal_number_of_clusters = ks_to_try[new_optimal_index]\n",
    "                    \n",
    "                    while (new_optimal_number_of_clusters != optimal_number_of_clusters) && (new_optimal_index != length(ks_to_try))\n",
    "                        optimal_number_of_clusters = new_optimal_number_of_clusters\n",
    "                        optimal_index = new_optimal_index\n",
    "                        optimal_silhouette = new_optimal_silhouette\n",
    "                        @info \"current optimal number of clusters = $(ks_to_try[optimal_index])\"\n",
    "                        @info \"current best silhouette score = $(optimal_silhouette)\"\n",
    "                        \n",
    "                        window_of_focus = ks_to_try[optimal_index-1:optimal_index+1]\n",
    "\n",
    "                        k_to_try = Int(round(Statistics.mean(window_of_focus[1:2])))\n",
    "                        insertion_index = first(searchsorted(ks_to_try, k_to_try))\n",
    "                        if ks_to_try[insertion_index] != k_to_try\n",
    "                            @info \"assessing k = $(k_to_try)\"\n",
    "                            this_clustering = Clustering.kmeans(distance_matrix, k_to_try)\n",
    "                            insert!(ks_to_try, insertion_index, k_to_try)\n",
    "                            insert!(within_cluster_sum_of_squares, insertion_index, wcss(this_clustering))\n",
    "                            insert!(silhouette_scores, insertion_index, Statistics.mean(Clustering.silhouettes(this_clustering, distance_matrix)))\n",
    "                        end\n",
    "\n",
    "                        k_to_try = Int(round(Statistics.mean(window_of_focus[2:3])))\n",
    "                        insertion_index = first(searchsorted(ks_to_try, k_to_try))\n",
    "                        if ks_to_try[insertion_index] != k_to_try\n",
    "                            @info \"assessing k = $(k_to_try)\"\n",
    "                            this_clustering = Clustering.kmeans(distance_matrix, k_to_try)\n",
    "                            insert!(ks_to_try, insertion_index, k_to_try)\n",
    "                            insert!(within_cluster_sum_of_squares, insertion_index, wcss(this_clustering))\n",
    "                            insert!(silhouette_scores, insertion_index, Statistics.mean(Clustering.silhouettes(this_clustering, distance_matrix)))\n",
    "                        end\n",
    "\n",
    "                        new_optimal_silhouette, new_optimal_index = findmax(silhouette_scores)\n",
    "                        new_optimal_number_of_clusters = ks_to_try[new_optimal_index]\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return optimal_number_of_clusters, ks_to_try, within_cluster_sum_of_squares, silhouette_scores\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assess_aamer_saturation (generic function with 3 methods)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function assess_aamer_saturation(fasta_records::AbstractVector{FASTX.FASTA.Record}, k; kmers_to_assess=Inf, power=10)\n",
    "    kmers = Set{BioSequences.LongAminoAcidSeq}()\n",
    "    \n",
    "    max_possible_kmers = length(generate_all_possible_kmers(k, Mycelia.AA_ALPHABET))\n",
    "    \n",
    "    if kmers_to_assess == Inf\n",
    "        kmers_to_assess = max_possible_kmers\n",
    "    end\n",
    "    \n",
    "    sampling_points = Int[0]\n",
    "    i = 0\n",
    "    while power^i <= kmers_to_assess\n",
    "        push!(sampling_points, power^i)\n",
    "        i += 1\n",
    "    end\n",
    "    \n",
    "    unique_kmer_counts = zeros(Int, length(sampling_points))\n",
    "    \n",
    "    if length(sampling_points) < 3\n",
    "        @info \"increase the # of reads analyzed or decrease the power to acquire more data points\"\n",
    "        return (;sampling_points, unique_kmer_counts)\n",
    "    end\n",
    "    \n",
    "    p = ProgressMeter.Progress(kmers_to_assess, 1)\n",
    "    \n",
    "    kmers_assessed = 0\n",
    "    for record in fasta_records\n",
    "        # for kmer in BioSequences.each(kmer_type, FASTX.sequence(record))\n",
    "        for i in 1:length(FASTX.sequence(record))-k+1\n",
    "            kmer = FASTX.sequence(record)[i:i+k-1]\n",
    "            push!(kmers, kmer)\n",
    "            kmers_assessed += 1\n",
    "            if (length(kmers) == max_possible_kmers)                 \n",
    "                sampling_points = vcat(filter(s -> s < kmers_assessed, sampling_points), [kmers_assessed])\n",
    "                unique_kmer_counts = vcat(unique_kmer_counts[1:length(sampling_points)-1], length(kmers))\n",
    "                return (;sampling_points, unique_kmer_counts, eof = false)\n",
    "            elseif kmers_assessed in sampling_points\n",
    "                i = findfirst(sampling_points .== kmers_assessed)\n",
    "                unique_kmer_counts[i] = length(kmers)\n",
    "                if i == length(sampling_points)\n",
    "                    return (sampling_points = sampling_points, unique_kmer_counts = unique_kmer_counts, eof = false)\n",
    "                end\n",
    "            end\n",
    "            ProgressMeter.next!(p)\n",
    "        end\n",
    "    end\n",
    "    sampling_points = vcat(filter(s -> s < kmers_assessed, sampling_points), [kmers_assessed])\n",
    "    unique_kmer_counts = vcat(unique_kmer_counts[1:length(sampling_points)-1], [length(kmers)])    \n",
    "    return (sampling_points = sampling_points, unique_kmer_counts = unique_kmer_counts, eof = true)\n",
    "end\n",
    "\n",
    "\n",
    "function assess_aamer_saturation(fastxs::AbstractVector{String}, k; kmers_to_assess=Inf, power=10)\n",
    "    kmers = Set{BioSequences.LongAminoAcidSeq}()\n",
    "    \n",
    "    max_possible_kmers = length(generate_all_possible_kmers(k, Mycelia.AA_ALPHABET))\n",
    "    \n",
    "    if kmers_to_assess == Inf\n",
    "        kmers_to_assess = max_possible_kmers\n",
    "    end\n",
    "    \n",
    "    sampling_points = Int[0]\n",
    "    i = 0\n",
    "    while power^i <= kmers_to_assess\n",
    "        push!(sampling_points, power^i)\n",
    "        i += 1\n",
    "    end\n",
    "    \n",
    "    unique_kmer_counts = zeros(Int, length(sampling_points))\n",
    "    \n",
    "    if length(sampling_points) < 3\n",
    "        @info \"increase the # of reads analyzed or decrease the power to acquire more data points\"\n",
    "        return (;sampling_points, unique_kmer_counts)\n",
    "    end\n",
    "    \n",
    "    p = ProgressMeter.Progress(kmers_to_assess, 1)\n",
    "    \n",
    "    kmers_assessed = 0\n",
    "    for fastx in fastxs\n",
    "        for record in Mycelia.open_fastx(fastx)\n",
    "            # for kmer in BioSequences.each(kmer_type, FASTX.sequence(record))\n",
    "            for i in 1:length(FASTX.sequence(record))-k+1\n",
    "                kmer = FASTX.sequence(record)[i:i+k-1]\n",
    "                push!(kmers, kmer)\n",
    "                kmers_assessed += 1\n",
    "                if (length(kmers) == max_possible_kmers)                 \n",
    "                    sampling_points = vcat(filter(s -> s < kmers_assessed, sampling_points), [kmers_assessed])\n",
    "                    unique_kmer_counts = vcat(unique_kmer_counts[1:length(sampling_points)-1], length(kmers))\n",
    "                    return (;sampling_points, unique_kmer_counts, eof = false)\n",
    "                elseif kmers_assessed in sampling_points\n",
    "                    i = findfirst(sampling_points .== kmers_assessed)\n",
    "                    unique_kmer_counts[i] = length(kmers)\n",
    "                    if i == length(sampling_points)\n",
    "                        return (sampling_points = sampling_points, unique_kmer_counts = unique_kmer_counts, eof = false)\n",
    "                    end\n",
    "                end\n",
    "                ProgressMeter.next!(p)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    sampling_points = vcat(filter(s -> s < kmers_assessed, sampling_points), [kmers_assessed])\n",
    "    unique_kmer_counts = vcat(unique_kmer_counts[1:length(sampling_points)-1], [length(kmers)])    \n",
    "    return (sampling_points = sampling_points, unique_kmer_counts = unique_kmer_counts, eof = true)\n",
    "end\n",
    "\n",
    "function assess_aamer_saturation(fastxs; outdir=\"\", min_k=1, max_k=15, threshold=0.1)\n",
    "    \n",
    "    if isempty(outdir)\n",
    "        outdir = joinpath(pwd(), \"aamer-saturation\")\n",
    "    end\n",
    "    mkpath(outdir)\n",
    "    \n",
    "    ks = Primes.primes(min_k, max_k)\n",
    "    ks = min_k:max_k\n",
    "    minimum_saturation = Inf\n",
    "    midpoint = Inf\n",
    "    \n",
    "    \n",
    "    \n",
    "    for k in ks\n",
    "        kmers_to_assess = 10_000_000\n",
    "        sampling_points, kmer_counts, hit_eof = assess_aamer_saturation(fastxs, k, kmers_to_assess=kmers_to_assess)\n",
    "        @show sampling_points, kmer_counts, hit_eof\n",
    "        observed_midpoint_index = findfirst(i -> kmer_counts[i] > last(kmer_counts)/2, 1:length(sampling_points))\n",
    "        observed_midpoint = sampling_points[observed_midpoint_index]\n",
    "        initial_parameters = Float64[maximum(kmer_counts), observed_midpoint]\n",
    "        @time fit = LsqFit.curve_fit(Mycelia.calculate_v, sampling_points, kmer_counts, initial_parameters)\n",
    "        if hit_eof\n",
    "            inferred_maximum = last(kmer_counts)\n",
    "        else\n",
    "            inferred_maximum = max(Int(ceil(fit.param[1])), last(kmer_counts))\n",
    "        end\n",
    "\n",
    "        max_possible_kmers = length(generate_all_possible_kmers(k, Mycelia.AA_ALPHABET))\n",
    "        \n",
    "        inferred_midpoint = Int(ceil(fit.param[2]))\n",
    "        predicted_saturation = inferred_maximum / max_possible_kmers\n",
    "        @show k, predicted_saturation\n",
    "\n",
    "        p = StatsPlots.scatter(\n",
    "            sampling_points,\n",
    "            kmer_counts,\n",
    "            label=\"observed kmer counts\",\n",
    "            ylabel=\"# unique kmers\",\n",
    "            xlabel=\"# kmers assessed\",\n",
    "            title = \"sequencing saturation @ k = $k\",\n",
    "            legend=:outertopright,\n",
    "            size=(800, 400),\n",
    "            margins=3StatsPlots.PlotMeasures.mm\n",
    "            )\n",
    "        StatsPlots.hline!(p, [max_possible_kmers], label=\"absolute maximum\")\n",
    "        StatsPlots.hline!(p, [inferred_maximum], label=\"inferred maximum\")\n",
    "        StatsPlots.vline!(p, [inferred_midpoint], label=\"inferred midpoint\")\n",
    "        # xs = vcat(sampling_points, [last(sampling_points) * 2^i for i in 1:2])\n",
    "        xs = sort([sampling_points..., inferred_midpoint])\n",
    "        ys = Mycelia.calculate_v(xs, fit.param)\n",
    "        StatsPlots.plot!(\n",
    "            p,\n",
    "            xs,\n",
    "            ys,\n",
    "            label=\"fit trendline\")\n",
    "        display(p)\n",
    "        StatsPlots.savefig(p, joinpath(outdir, \"$k.png\"))\n",
    "        StatsPlots.savefig(p, joinpath(outdir, \"$k.svg\"))\n",
    "\n",
    "        if predicted_saturation < minimum_saturation\n",
    "            minimum_saturation = predicted_saturation\n",
    "            min_k = k\n",
    "            midpoint = inferred_midpoint \n",
    "        end\n",
    "        if predicted_saturation < threshold\n",
    "            chosen_k_file = joinpath(outdir, \"chosen_k.txt\")\n",
    "            println(\"chosen k = $k\")\n",
    "            open(chosen_k_file, \"w\") do io\n",
    "                println(io, k)\n",
    "            end\n",
    "            return k\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2733124"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?&id=$(tax_id)\n",
    "# https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?lvl=0&amp;id=2733124\n",
    "root_tax_id = 2733124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# child_tax_ids = vcat(Mycelia.taxonomic_id_to_children(root_tax_id), root_tax_id)\n",
    "# # child_tax_ids = vcat(child_tax_ids, root_tax_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# here is where we should apply a filter where host == Escherichia\n",
    "# need to load host information into neo4j taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # refseq_metadata = Mycelia.load_refseq_metadata()\n",
    "# ncbi_metadata = Mycelia.load_genbank_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show(ncbi_metadata[1:1, :], allcols=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tax_id_filter = map(taxid -> taxid in child_tax_ids, ncbi_metadata[!, \"taxid\"])\n",
    "# is_right_host = map(x -> occursin(r\"Escherichia\"i, x), ncbi_metadata[!, \"organism_name\"])\n",
    "# not_excluded = ncbi_metadata[!, \"excluded_from_refseq\"] .== \"\"\n",
    "# is_full = ncbi_metadata[!, \"genome_rep\"] .== \"Full\"\n",
    "# # assembly_levels = [\"Complete Genome\"]\n",
    "# assembly_levels = [\"Complete Genome\", \"Chromosome\"]\n",
    "# # assembly_levels = [\"Complete Genome\", \"Chromosome\", \"Scaffold\"]\n",
    "# # assembly_levels = [\"Complete Genome\", \"Chromosome\", \"Scaffold\", \"Contig\"]\n",
    "# assembly_level_filter = map(x -> x in assembly_levels, ncbi_metadata[!, \"assembly_level\"])\n",
    "# full_filter = is_full .& not_excluded .& assembly_level_filter .& tax_id_filter .& is_right_host\n",
    "# count(full_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# here is another place we could enforce host == escherichia\n",
    "# we'll use a manual filter as a temporary solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncbi_metadata_of_interest = ncbi_metadata[full_filter, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.ncbi.nlm.nih.gov/sviewer/viewer.cgi?db=nuccore&report=genbank&id=GCA_021354775"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in names(ncbi_metadata_of_interest)\n",
    "#     @show col, ncbi_metadata_of_interest[1, col]\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCA_002956955.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # can I also get genbank record?????\n",
    "# # for extension in [\"genomic.fna.gz\", \"protein.faa.gz\"]\n",
    "# for extension in [\"genomic.fna.gz\", \"protein.faa.gz\", \"genomic.gbff.gz\"]\n",
    "#     outdir = mkpath(joinpath(DIR, extension))\n",
    "#     ProgressMeter.@showprogress for row in DataFrames.eachrow(ncbi_metadata_of_interest)\n",
    "#         url = Mycelia.ncbi_ftp_path_to_url(row[\"ftp_path\"], extension)\n",
    "#         outfile = joinpath(outdir, basename(url))\n",
    "#         if !isfile(outfile)\n",
    "#             try\n",
    "#                 Downloads.download(url, outfile)\n",
    "#             catch e\n",
    "#                 # @show e\n",
    "#                 showerror(stdout, e)\n",
    "#                 # @assert extension == \"protein.faa.gz\"\n",
    "#                 # here is where we should call prodigal to fill in protein annotations if we don't otherwise see them\n",
    "#             end\n",
    "#         end\n",
    "#     end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extension = \"protein.faa.gz\"\n",
    "outdir = mkpath(joinpath(DIR, extension))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastx_files = filter(x -> !occursin(\".ipynb_checkpoints\", x), readdir(outdir, join=true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This section generates a distance matrix for the fasta files i.e. the protein profile of the entire genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # these are too small, all of the within vs between have some disagreement\n",
    "# # dna_k = 5\n",
    "# aa_k = 2\n",
    "# # should use these?\n",
    "# # dna_k = 7\n",
    "# # aa_k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_table, outfile = Mycelia.fasta_list_to_counts_table(fasta_list=fastx_files, k=aa_k, alphabet=:AA, outfile=\"$(outdir).$(aa_k).counts.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance_matrix = Mycelia.counts_matrix_to_distance_matrix(counts_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This section generates a distance matrix for the individual proteins, so we can find clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record_table = DataFrames.DataFrame(\n",
    "#     fastx_file = String[],\n",
    "#     record_identifier = String[],\n",
    "#     record_description = String[]\n",
    "# )\n",
    "# ProgressMeter.@showprogress for fastx_file in fastx_files\n",
    "#     for record in Mycelia.open_fastx(fastx_file)\n",
    "#         row = (\n",
    "#             fastx_file = fastx_file,\n",
    "#             record_identifier = FASTX.identifier(record),\n",
    "#             record_description = FASTX.description(record)\n",
    "#         )\n",
    "#         push!(record_table, row)\n",
    "#     end\n",
    "# end\n",
    "# record_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphabet = :AA\n",
    "# k = aa_k\n",
    "# fasta_list = fastx_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if alphabet == :AA\n",
    "#     canonical_mers = Mycelia.generate_all_possible_canonical_kmers(k, Mycelia.AA_ALPHABET)\n",
    "# elseif alphabet == :DNA\n",
    "#     canonical_mers = Mycelia.generate_all_possible_canonical_kmers(k, Mycelia.DNA_ALPHABET)\n",
    "# else\n",
    "#     error(\"invalid alphabet\")\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if isempty(outfile)\n",
    "# outfile = joinpath(pwd(), \"$(hash(fasta_list)).$(alphabet).k$(k).by-record.bin\")\n",
    "# # end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #     # if isfile(outfile)\n",
    "# # load into memory\n",
    "# # mer_counts_matrix = Mmap.mmap(open(outfile), Array{Int, 2}, (length(canonical_mers), DataFrames.nrow(record_table)))\n",
    "# # else\n",
    "# # start from scratch\n",
    "# mer_counts_matrix = Mmap.mmap(open(outfile, \"w+\"), Array{Int, 2}, (length(canonical_mers), DataFrames.nrow(record_table)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function count_aamers(k, fasta_protein::FASTX.FASTA.Record)\n",
    "#     s = FASTX.sequence(fasta_protein)\n",
    "#     these_counts = sort(StatsBase.countmap([s[i:i+k-1] for i in 1:length(s)-k-1]))\n",
    "#     return these_counts    \n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# p = ProgressMeter.Progress(DataFrames.nrow(record_table))\n",
    "# # ProgressMeter.@showprogress for fastx_file in fastx_files\n",
    "# for fastx_file in fastx_files\n",
    "#     for record in Mycelia.open_fastx(fastx_file)\n",
    "#         i += 1\n",
    "#         @assert fastx_file == record_table[i, \"fastx_file\"]\n",
    "#         @assert FASTX.identifier(record) == record_table[i, \"record_identifier\"]\n",
    "#         @assert FASTX.description(record) == record_table[i, \"record_description\"]\n",
    "#         ProgressMeter.next!(p)\n",
    "#         # entity_mer_counts = Mycelia.count_aamers(k, record)\n",
    "#         entity_mer_counts = count_aamers(k, record)\n",
    "#         Mycelia.update_counts_matrix!(mer_counts_matrix, i, entity_mer_counts, canonical_mers)\n",
    "#     end\n",
    "# end\n",
    "# mer_counts_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, distance matrix\n",
    "# distance_matrix = Mycelia.counts_matrix_to_distance_matrix(mer_counts_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c bioconda diamond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run(`diamond help`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_fasta_outfile = outdir * \".joint.faa.gz\"\n",
    "if !isfile(joint_fasta_outfile)\n",
    "    open(joint_fasta_outfile, \"w\") do io\n",
    "        for fastx_file in fastx_files\n",
    "            write(io, read(fastx_file))\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @time run(`diamond makedb --in $(joint_fasta_outfile) -d $(joint_fasta_outfile)`)\n",
    "\n",
    "# N_RECORDS = DataFrames.nrow(record_table)\n",
    "# # qseqid sseqid pident length mismatch gapopen qlen qstart qend slen sstart send evalue bitscore\n",
    "\n",
    "# blastp_header = [\n",
    "#     \"qseqid\",\n",
    "#     \"sseqid\",\n",
    "#     \"pident\",\n",
    "#     \"length\",\n",
    "#     \"mismatch\",\n",
    "#     \"gapopen\",\n",
    "#     \"qlen\",\n",
    "#     \"qstart\",\n",
    "#     \"qend\",\n",
    "#     \"slen\",\n",
    "#     \"sstart\",\n",
    "#     \"send\",\n",
    "#     \"evalue\",\n",
    "#     \"bitscore\"\n",
    "# ]\n",
    "\n",
    "# # --fast                   enable fast mode\n",
    "# # --mid-sensitive          enable mid-sensitive mode\n",
    "# # --sensitive              enable sensitive mode)\n",
    "# # --more-sensitive         enable more sensitive mode\n",
    "# # --very-sensitive         enable very sensitive mode\n",
    "# # --ultra-sensitive        enable ultra sensitive mode\n",
    "# # --iterate                iterated search with increasing sensitivity\n",
    "\n",
    "# # TODO: pairwise output is all of the alignments, super helpful!\n",
    "\n",
    "# # running a search in blastp mode\n",
    "# # ./diamond blastp -d reference -q queries.fasta -o matches.tsv\n",
    "# # @time run(`diamond blastp --outfmt 0 -d $(joint_fasta_outfile).dmnd -q $(joint_fasta_outfile) -o $(joint_fasta_outfile).diamond.tsv`)\n",
    "# # @time run(`diamond blastp --sensitive -d $(joint_fasta_outfile).dmnd -q $(joint_fasta_outfile) -o $(joint_fasta_outfile).diamond.tsv`)\n",
    "# # @time run(`diamond blastp --iterate --id 0 --min-score 0 --max-target-seqs $(N_RECORDS) --unal 1 --outfmt 6 qseqid sseqid pident length mismatch gapopen qlen qstart qend slen sstart send evalue bitscore -d $(joint_fasta_outfile).dmnd -q $(joint_fasta_outfile) -o $(joint_fasta_outfile).diamond.tsv`)\n",
    "# @time run(`diamond blastp --ultra-sensitive --id 0 --min-score 0 --max-target-seqs $(N_RECORDS) --unal 1 --outfmt 6 qseqid sseqid pident length mismatch gapopen qlen qstart qend slen sstart send evalue bitscore -d $(joint_fasta_outfile).dmnd -q $(joint_fasta_outfile) -o $(joint_fasta_outfile).diamond.tsv`)\n",
    "# @time run(`diamond blastp --ultra-sensitive --id 0 --min-score 0 --max-target-seqs $(N_RECORDS) --unal 1 --outfmt 0  -d $(joint_fasta_outfile).dmnd -q $(joint_fasta_outfile) -o $(joint_fasta_outfile).diamond.pairwise.txt`)\n",
    "\n",
    "# # iterate\n",
    "# # Total time = 1.16s\n",
    "# # Reported 46718 pairwise alignments, 46718 HSPs.\n",
    "# # sensitive\n",
    "# # Total time = 5.673s\n",
    "# # Reported 49976 pairwise alignments, 49976 HSPs.\n",
    "# # ultra sensitive\n",
    "# # Total time = 14.939s\n",
    "# # Reported 52446 pairwise alignments, 52446 HSPs.\n",
    "\n",
    "# blastp_results = DataFrames.DataFrame(uCSV.read(\"$(joint_fasta_outfile).diamond.tsv\", header=0, delim='\\t', typedetectrows=100)[1], blastp_header)\n",
    "\n",
    "# uCSV.write(\"$(joint_fasta_outfile).diamond.with_header.tsv\", blastp_results, delim='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_to_index_map = Dict(identifier => i for (i, identifier) in enumerate(record_table[!, \"record_identifier\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show(blastp_results, allcols=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance_matrix = ones(N_RECORDS, N_RECORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for row in DataFrames.eachrow(blastp_results)\n",
    "#     row_idx = id_to_index_map[row[\"qseqid\"]]\n",
    "#     col_idx = id_to_index_map[row[\"sseqid\"]]\n",
    "#     # distance = 1 - (row[\"pident\"] / 100)\n",
    "#     sequence_identity = row[\"pident\"] / 100\n",
    "#     size_identity = row[\"length\"] / max(row[\"qlen\"], row[\"slen\"])\n",
    "#     overall_identity = sequence_identity * size_identity\n",
    "#     distance = 1 - (overall_identity)\n",
    "#     distance_matrix[row_idx, col_idx] = distance\n",
    "# end\n",
    "# distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just percent identity\n",
    "# Summary Stats:\n",
    "# Length:         13942756\n",
    "# Missing Count:  0\n",
    "# Mean:           0.996645\n",
    "# Minimum:        0.000000\n",
    "# 1st Quartile:   1.000000\n",
    "# Median:         1.000000\n",
    "# 3rd Quartile:   1.000000\n",
    "# Maximum:        1.000000\n",
    "# Type:           Float64\n",
    "\n",
    "# percent size and percent identity\n",
    "# Summary Stats:\n",
    "# Length:         13942756\n",
    "# Missing Count:  0\n",
    "# Mean:           0.996742\n",
    "# Minimum:        0.000000\n",
    "# 1st Quartile:   1.000000\n",
    "# Median:         1.000000\n",
    "# 3rd Quartile:   1.000000\n",
    "# Maximum:        1.000000\n",
    "# Type:           Float64\n",
    "\n",
    "# StatsBase.describe(vec(distance_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal_number_of_clusters, ks_assessed, within_cluster_sum_of_squares, silhouette_scores = fit_optimal_number_of_clusters(distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p1 = StatsPlots.plot(\n",
    "#     ks_assessed[1:length(within_cluster_sum_of_squares)],\n",
    "#     within_cluster_sum_of_squares,\n",
    "#     ylabel = \"within cluster sum of squares\\n(lower is better)\",\n",
    "#     xlabel = \"n clusters\",\n",
    "#     title = \"Optimal n clusters = $(optimal_number_of_clusters)\",\n",
    "#     legend=false\n",
    "# )\n",
    "# StatsPlots.vline!(p1, [optimal_number_of_clusters])\n",
    "# p2 = StatsPlots.plot(\n",
    "#     ks_assessed[1:length(silhouette_scores)],\n",
    "#     silhouette_scores,\n",
    "#     ylabel = \"silhouette scores\\n(higher is better)\",\n",
    "#     xlabel = \"n clusters\",\n",
    "#     title = \"Optimal n clusters = $(optimal_number_of_clusters)\",\n",
    "#     legend=false\n",
    "# )\n",
    "# StatsPlots.vline!(p2, [optimal_number_of_clusters])\n",
    "# display(p1)\n",
    "# display(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal_clustering_result = Clustering.kmeans(distance_matrix, optimal_number_of_clusters)\n",
    "# record_table[!, \"cluster_assignments\"] = optimal_clustering_result.assignments\n",
    "# show(record_table, allcols=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_clusters = sort(collect(StatsBase.countmap(record_table[!, \"cluster_assignments\"])), by=x->x[2], rev=true)\n",
    "\n",
    "# cluster_descriptions = DataFrames.DataFrame(\n",
    "#     cluster_id = Int[],\n",
    "#     cluster_count = Int[],\n",
    "#     cluster_description = String[]\n",
    "# )\n",
    "# for cluster in first.(sorted_clusters)\n",
    "#     word_cloud = Dict{String, Int}()\n",
    "#     cluster_indices = findall(record_table[!, \"cluster_assignments\"] .== cluster)\n",
    "#     for row in DataFrames.eachrow(record_table[cluster_indices, DataFrames.Not(\"fastx_file\")])\n",
    "#         # @show row[\"record_identifier\"]\n",
    "#         # @show row[\"record_description\"]\n",
    "#         filtered_description = replace(row[\"record_description\"], r\"\\[.*?\\]$\" => \"\")\n",
    "#         # @show filtered_description\n",
    "#         merge!(+, word_cloud, StatsBase.countmap(split(lowercase(filtered_description))))\n",
    "#     end\n",
    "#     word_cloud\n",
    "\n",
    "#     word_cloud = sort(collect(word_cloud), by=x->x[2], rev=true)\n",
    "\n",
    "#     if length(word_cloud) > 1\n",
    "#         word_cloud = filter(x -> x[2] > 1, word_cloud)\n",
    "#     end\n",
    "#     uninformative_words = [\n",
    "#         \"hypothetical\",\n",
    "#         \"putative\",\n",
    "#         \"protein\",\n",
    "#         \"of\"\n",
    "#     ]\n",
    "#     word_cloud = filter(x -> !(x[1] in uninformative_words), word_cloud)\n",
    "#     # filter out any words that are substrings of other words (e.g. sir2 is a substring of sir2-like)\n",
    "#     word_cloud = filter(x -> !any(y -> x[1] != y[1] && occursin(x[1], y[1]), word_cloud), word_cloud)\n",
    "\n",
    "#     joint_descriptor = join(first.(word_cloud), \" \")\n",
    "#     if isempty(joint_descriptor)\n",
    "#         joint_descriptor = \"hypothetical protein of uknown function\"\n",
    "#     end\n",
    "#     row = (\n",
    "#         cluster_id = cluster,\n",
    "#         cluster_count = length(cluster_indices),\n",
    "#         cluster_description = joint_descriptor\n",
    "#     )\n",
    "#     push!(cluster_descriptions, row)\n",
    "# end\n",
    "\n",
    "# show(cluster_descriptions[cluster_descriptions[!, \"cluster_description\"] .!= \"hypothetical protein of uknown function\", :], allrows=true, allcols=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function term_frequency(documents)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function document_frequency(documents)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function tf_idf(document_groups)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap of clusters against genomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record_table[!, [\"fastx_file\", \"cluster_assignments\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_fastas = length(fastx_files)\n",
    "# n_clusters = optimal_number_of_clusters\n",
    "# fasta_cluster_containment_matrix = falses(n_fastas, n_clusters)\n",
    "\n",
    "# for (i, fastx_file_group) in enumerate(DataFrames.groupby(record_table, \"fastx_file\"))\n",
    "#     clusters_contained = unique(fastx_file_group[!, \"cluster_assignments\"])\n",
    "#     for cluster in clusters_contained\n",
    "#         fasta_cluster_containment_matrix[i, cluster] = true\n",
    "#     end\n",
    "# end\n",
    "\n",
    "# clusters_ordered_by_coreness = sortperm(map(col -> sum(col), eachcol(fasta_cluster_containment_matrix)), rev=true)\n",
    "# StatsPlots.heatmap(\n",
    "#     fasta_cluster_containment_matrix[:, clusters_ordered_by_coreness],\n",
    "#     # legend = false,\n",
    "#     title = \"Core and accessory protein clusters\",\n",
    "#     ylabel = \"genome index\",\n",
    "#     xlabel = \"ordered protein clusters\",\n",
    "#     yticks = false,\n",
    "#     xticks = false,\n",
    "#     margins = 1StatsPlots.cm\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names(record_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record_table[!, \"cluster_assignments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joint_table = DataFrames.innerjoin(record_table, cluster_descriptions, on=\"cluster_assignments\" => \"cluster_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joint_table[!, \"cluster_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joint_table[!, \"cluster_frequency\"] = joint_table[!, \"cluster_count\"] ./ n_fastas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uCSV.write(\"$(joint_fasta_outfile).protein_clusters.tsv\", joint_table, delim='\\t')\n",
    "joint_table = DataFrames.DataFrame(uCSV.read(\"$(joint_fasta_outfile).protein_clusters.tsv\", delim='\\t', header=1)...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE CONSUSES PROTEIN FOR EACH PROTEIN CLUSTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for cluster in DataFrames.groupby(join_table, \"cluster_assignments\")\n",
    "# cluster = first(DataFrames.groupby(joint_table, \"cluster_assignments\"))\n",
    "# show(cluster, allcols=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = sort(unique(joint_table[!, \"cluster_assignments\"]))\n",
    "cluster_fasta_files = [replace(joint_fasta_outfile, \".faa.gz\" => \"\") .* \".cluster_$(cluster).faa\" for cluster in clusters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each cluster, write out cluster to a specific fasta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_fasta_ios = [FASTX.FASTA.Writer(open(f, \"w\")) for f in cluster_fasta_files]\n",
    "for record in Mycelia.open_fastx(joint_fasta_outfile)\n",
    "    [FASTX.identifier(record)]\n",
    "    record_index = findfirst(joint_table[!, \"record_identifier\"] .== FASTX.identifier(record))\n",
    "    cluster_assignment = joint_table[record_index, \"cluster_assignments\"]\n",
    "    cluster_io = cluster_fasta_ios[cluster_assignment]\n",
    "    write(cluster_io, record)\n",
    "end\n",
    "for io_stream in cluster_fasta_ios\n",
    "    close(io_stream)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_k = assess_aamer_saturation([joint_fasta_outfile], threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out clustalw alignment for each fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProgressMeter.@showprogress for cluster_fasta_file in cluster_fasta_files\n",
    "    # for outfmt in [\"fasta\", \"clustal\", \"msf\", \"phylip\", \"selex\", \"stockholm\", \"vienna\"]\n",
    "    for outfmt in [\"clustal\"]\n",
    "        outfile = \"$(cluster_fasta_file).clustal_omega.$(outfmt)\"\n",
    "        if !isfile(outfile)\n",
    "            try\n",
    "                run(`clustalo -i $(cluster_fasta_file) --outfmt $(outfmt) -o $(outfile)`)\n",
    "            catch e\n",
    "                # FATAL: File '...' contains 1 sequence, nothing to align\n",
    "                continue\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastx_to_aamer_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read in the list of fastas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count aamers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize graph with aamer nodes and counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges have weights too?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
